{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "## What will you need?\n",
        "1. Hugging Face Account and Authentication Token (Free)\n",
        "2. Ngrok Account and Authentication Token (Free)\n",
        "3. For Accessing the Llama Model, you will need a permission, since it is a gated model. Fill [this](https://llama.meta.com/llama-downloads/) form, and you should get the permission to access the model from Hugging Face within 1/2 hr. (Free)\n",
        "4. A Dataset of what you want your model to learn.\n",
        "\n",
        "    a. You can use Open source datasets\n",
        "    \n",
        "    b. Or Create your own! (Process mentioned below)"
      ],
      "metadata": {
        "id": "1DCq6GouZTKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installations**\n",
        "Run **this** every time you restart your session"
      ],
      "metadata": {
        "id": "mSBkmLeWZWJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation for QLORA\n",
        "!pip install -q -U bitsandbytes\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install transformers==4.31 #temporary fix required owing to breaking changes on Aug 9th 2023\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "oEZwLbRrdiIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa0d335-3fde-42fb-cafc-21bf055c70f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.31\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.2\n",
            "    Uninstalling transformers-4.37.2:\n",
            "      Successfully uninstalled transformers-4.37.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.31.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OpK0huz9ZJnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec58805-7c42-4a1f-d73b-75999dd93a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/817.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/817.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/817.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m809.0/817.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.6/171.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chainlit 1.0.301 requires python-multipart<0.0.7,>=0.0.6, but you have python-multipart 0.0.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installation for Frontend and Langchain\n",
        "!pip install langchain langchain_experimental openai --quiet # certain libraries require openai to be installed\n",
        "!pip install chainlit langchain_community pyngrok --quiet\n",
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  ctransformers"
      ],
      "metadata": {
        "id": "TtILH4VDPHrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97882ba7-90b0-4185-82e7-649af39f1896"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creation of Dataset**\n"
      ],
      "metadata": {
        "id": "TY6072lmdifg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For finetuning, you require a dataset on which the model needs to be trained. For our usecase, we created our own dataset using the following steps.\n",
        "\n",
        "1. Extraction of data (From websites, blogs, etc.)\n",
        "2. Formatting Data in Google docs\n",
        "3. Creating Question and Answer Pairs using that data (using ChatGPT manually)\n",
        "4. Checking all question and answer pairs (Google Docs)\n",
        "5. Writing Python script to convert the question and answer pairs into a jsonl file, which will be used for further finetuning"
      ],
      "metadata": {
        "id": "agagi144EaSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Q9zITyK7vrkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_qa_pairs(text):\n",
        "    # Regular expressions to identify questions and answers - EDIT THIS ACCORDING TO YOUR TRAINING DATA FORMAT\n",
        "    question_pattern = r\"\\*\\*Question\\*\\*:\\s*(.*?)\\n\"\n",
        "    answer_pattern = r\"\\*\\*Answer\\*\\*:\\s*(.*?)\\n\"\n",
        "\n",
        "    # Find all matches in the text\n",
        "    questions = re.findall(question_pattern, text, re.DOTALL)\n",
        "    answers = re.findall(answer_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Combine questions and answers into a list of dictionaries\n",
        "    qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(questions, answers)]\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# Read your text data from a file (replace 'your_file.txt' with your file path)\n",
        "with open('/content/Training.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Parse the question-answer pairs\n",
        "qa_pairs = parse_qa_pairs(text)\n",
        "\n",
        "# Convert to JSONL and write to a file\n",
        "with open('output.jsonl', 'w') as file:\n",
        "    for pair in qa_pairs:\n",
        "        json_line = json.dumps(pair)\n",
        "        file.write(json_line + \"\\n\")"
      ],
      "metadata": {
        "id": "H3WtTnSBeqBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_json(\"/content/output.jsonl\", lines = True)\n",
        "data"
      ],
      "metadata": {
        "id": "XZyLSjWxetV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "6385c87c-0f19-4486-ee2e-9e4e0ac54b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               question  \\\n",
              "0     What does ESG stand for in the context of Mait...   \n",
              "1     What is Maitri Lab Grown Diamond's goal in ter...   \n",
              "2     How does Maitri Lab Grown Diamond emphasize su...   \n",
              "3     What are the key priorities under Maitri Lab G...   \n",
              "4     How does Maitri Lab Grown Diamond reduce its c...   \n",
              "...                                                 ...   \n",
              "1015  What steps has Maitri Labgrown Diamonds taken ...   \n",
              "1016  How does Maitri Labgrown Diamonds contribute t...   \n",
              "1017  What role does the SCS-007 sustainability cert...   \n",
              "1018  How does Maitri Labgrown Diamonds' commitment ...   \n",
              "1019  Can you explain the process of planting a tree...   \n",
              "\n",
              "                                                 answer  \n",
              "0     ESG stands for Environmental, Social, and Gove...  \n",
              "1     Maitri Lab Grown Diamond aims to become a net-...  \n",
              "2     Sustainability is at the core of Maitri Lab Gr...  \n",
              "3     The key priorities include environmental respo...  \n",
              "4     Maitri Lab Grown Diamond reduces its carbon fo...  \n",
              "...                                                 ...  \n",
              "1015  Maitri Labgrown Diamonds ensures ethical sourc...  \n",
              "1016  Maitri Labgrown Diamonds mitigates the environ...  \n",
              "1017  The SCS-007 sustainability certificate helps M...  \n",
              "1018  Maitri Labgrown Diamonds' CSR commitment align...  \n",
              "1019  Maitri Labgrown Diamonds plants a tree for eve...  \n",
              "\n",
              "[1020 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-406ceef3-0f53-4ee5-bcc4-3c0d295281e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What does ESG stand for in the context of Mait...</td>\n",
              "      <td>ESG stands for Environmental, Social, and Gove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is Maitri Lab Grown Diamond's goal in ter...</td>\n",
              "      <td>Maitri Lab Grown Diamond aims to become a net-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does Maitri Lab Grown Diamond emphasize su...</td>\n",
              "      <td>Sustainability is at the core of Maitri Lab Gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the key priorities under Maitri Lab G...</td>\n",
              "      <td>The key priorities include environmental respo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Maitri Lab Grown Diamond reduce its c...</td>\n",
              "      <td>Maitri Lab Grown Diamond reduces its carbon fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>What steps has Maitri Labgrown Diamonds taken ...</td>\n",
              "      <td>Maitri Labgrown Diamonds ensures ethical sourc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>How does Maitri Labgrown Diamonds contribute t...</td>\n",
              "      <td>Maitri Labgrown Diamonds mitigates the environ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>What role does the SCS-007 sustainability cert...</td>\n",
              "      <td>The SCS-007 sustainability certificate helps M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>How does Maitri Labgrown Diamonds' commitment ...</td>\n",
              "      <td>Maitri Labgrown Diamonds' CSR commitment align...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>Can you explain the process of planting a tree...</td>\n",
              "      <td>Maitri Labgrown Diamonds plants a tree for eve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-406ceef3-0f53-4ee5-bcc4-3c0d295281e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-406ceef3-0f53-4ee5-bcc4-3c0d295281e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-406ceef3-0f53-4ee5-bcc4-3c0d295281e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a1ea20f8-498f-4805-8fda-e71ad0771c5a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a1ea20f8-498f-4805-8fda-e71ad0771c5a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a1ea20f8-498f-4805-8fda-e71ad0771c5a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4c2ce6d6-8521-4b58-b208-6736c35a82e9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4c2ce6d6-8521-4b58-b208-6736c35a82e9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1020,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 984,\n        \"samples\": [\n          \"What is the primary focus of Maitri Lab-Grown Diamonds in terms of providing choice to consumers?\",\n          \"How can consumers ensure they are purchasing genuine lab-grown diamonds?\",\n          \"What are some common misconceptions about lab-grown diamonds that need to be addressed?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 990,\n        \"samples\": [\n          \"The production process of lab-grown diamonds ensures a lower carbon footprint by using less energy-intensive methods and renewable energy sources, compared to the extensive and often harmful environmental impact of traditional diamond mining.\",\n          \"Lab Grown Diamonds mitigate biodiversity loss by preventing habitat destruction and environmental disruption associated with diamond mining.\",\n          \"Maitri Lab Grown Diamonds aligns its commitment to sustainability by using renewable energy, minimizing resource usage, and recycling materials in the diamond-growing process.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_len_list = []\n",
        "for item in data['question']:\n",
        "  Q_len_list.append(len(item.strip()))\n",
        "\n",
        "A_len_list = []\n",
        "for item in data['answer']:\n",
        "  A_len_list.append(len(item.strip()))"
      ],
      "metadata": {
        "id": "kIFr3nl9e0xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the length of each question and answer\n",
        "question_lengths = Q_len_list\n",
        "answer_lengths = A_len_list\n",
        "\n",
        "# Plotting the distribution of question and answer lengths\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(question_lengths, bins=20, color='blue', alpha=0.7)\n",
        "plt.title('Distribution of Question Lengths')\n",
        "plt.xlabel('Length of Question (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(answer_lengths, bins=20, color='green', alpha=0.7)\n",
        "plt.title('Distribution of Answer Lengths')\n",
        "plt.xlabel('Length of Answer (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fvQFhBg4e2lR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "e43fca02-d859-4065-f390-d0ad845261a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Llama 2**\n"
      ],
      "metadata": {
        "id": "DIiGJP0gfHsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "CJFPYhiig4vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "RWRJPh-61Q6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantisation Config (explanation in next section)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "PLRRtoM4EleU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "9147f74e30ae4f5aa837debcf0a99258",
            "19115ccc9bda425b99f92fdf64ab71af",
            "ab1e64feb5df4156bc9ed7f97bf9aac0",
            "7036174a79f044de9b3e20bd15b3b088",
            "ee8017b6b85942c6a00081111f66323d",
            "757a868ca6f540b6b5b425360209695c",
            "019a424d43904b84b013e884af1c5e09",
            "43c6fc6d2b0e4174a0737892ab353d45",
            "42bbbb17ebde426785c98c18093c0318",
            "6312c70c17ff419da2a7821201f254dc",
            "7eefbbf215bf4ee2a31a57359a5b1194",
            "2639e65b802040798841b27a97e7c6dc",
            "8d85fb7223f44941994bca16837ddee8",
            "3509f695cc0b4cae970ebc0dd8234543",
            "23fddd1b5e984ee88221972cfdea4741",
            "92d4e1d1bd194336bba177e5acf0df0f",
            "c806681591c245eaa62b45c8b1f46e21",
            "82c4c3b7d1b1401fa54f283e85f13b97",
            "6f4b79dcbdda40648c35bcc51d4e3608",
            "005358959a1646b6a30ed3733913159b",
            "23955802bff14e029ab8b5fc1c8a69dd",
            "49cc5e940ad540e99cdf327a447e9684",
            "71d24a7e0b7749218f990365ba892517",
            "6d06b6d4c0b047e6b3fa84274b0087e0",
            "cd03f03ee3d8491b9d2c6b8fa8b82794",
            "a94ae04343ec4486aa59f65e688aba09",
            "04654b7d0551494eb4944962774e588a",
            "f3015eb812ce4fd1a6324c084b9d4853",
            "3798962f4edc4529924f45b5364665ce",
            "2fa85e095a7d4fe4ba77b07b70e6bbb4",
            "f7961a9d8ed647e789f52e9ec8555d61",
            "edc712a81fd6412fb31091601830d04b"
          ]
        },
        "id": "8--FXG4v6DlL",
        "outputId": "879b861d-06db-4f23-c8bb-c963ce88a8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9147f74e30ae4f5aa837debcf0a99258"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put your token in the output of the above cell"
      ],
      "metadata": {
        "id": "fvvlKwn86NV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A tokenizer is in charge of preparing the inputs for a model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Loading the actual model from Hugging Face\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "56nStMprEuss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "f5be36330f204a84951ff0c11b2c3d2c",
            "3f291a563b324c469c8159fd51515fe6",
            "f4c0dae27bbe471b8218701d0c8b4e97",
            "0f25c6c9f9464c1cbd376f1fe30206bd",
            "1a5004efe605498a9ab03aa43b6af2f5",
            "23a23c6749e74fc78b1dad13af6dfbce",
            "4478f5bdcc1e4856be8e0202998ae9bb",
            "60c25d7273d64b048f8a4da0fb558f42",
            "ceb537b3b2804b00b0d443441520be8f",
            "29cd2fc61af941b7a88735d875606381",
            "563d453c12f8407da5d55fc985224e86",
            "9b647a5b03e04f9a83d8c6c3bcc8d0bd",
            "044e9fafddba4956a826c6807a6ec125",
            "d379749af6a94970b04cc0b1516c2513",
            "26fd3e860e734024924781356f0d801b",
            "a7f419dc76ba4eeb9300e4582518f962",
            "d04530f6c958442bb25adcdfa96b9deb",
            "1890f432f3fa4a1cbf2d36b52c5e0390",
            "64854930553645de9e6d35eac673088c",
            "a3a062dd08ce447ca63dc10b925a7866",
            "cb812083a3f646eb88fbe6a4a4fb4e71",
            "4c42bfab6e3d4ba8b5a2cf686784caf6",
            "178bd0f522764d1d92cfa6d06f3bd918",
            "52cb6367e608499e8b6d49e1859445b3",
            "2043f730376b496794a8e7a1c47c9658",
            "e9dabe5955b94c4fafc4739ae3a8c603",
            "51223ac3fc83416fb83f60e3c1649398",
            "c8c6b90798664c1da324508822a43a04",
            "1d290484bf564d62a3521fd2882b79c3",
            "e457c0028ffa4bf9b488d27f13523d61",
            "0a55955ec42f483ab5f3c1b9b9a44aa5",
            "0c18acd21bb740c083cd33e16de8180a",
            "fcef8bff088a4467a63cb2481aa72721",
            "c857d28cb16644d2ba23d48e41869ae8",
            "1f5a92378cba4075add6a88dc8a653d2",
            "1ceb52c6c64c4808ab7fefa9930fd8b5",
            "ee6c1fd99efd427cb46742b4a1d596f9",
            "a2b196c6a6784415b455bed74b3500ad",
            "6dec7bba36b341a88c5a7641997074c6",
            "d441732911a34778848244adb5958a61",
            "17e16a14da84431f8e5f7a4eb4564555",
            "a03b84fd2b0946a386cb4d130f67362a",
            "789adcc815114c6984b60b0fdcf8f345",
            "b3028fdf9c844050b5d76dbd4f261a65",
            "5f1511b532634b0897cafb37aae3c0ed",
            "74a39bde1b7b4ab9b53bfb7125d39760",
            "f3e38e9b8506429a99eaa36d97b9baa1",
            "384ed87a142c4a1ab7853b43845855d0",
            "f83061a0e1c043fcb79cdeb7a25f460e",
            "d2667a2d79c74d2893ad4b48932801fb",
            "4cb63b2001364d7a9e39bc2b44c7a724",
            "dee4b8b66a5d4fbb8a14b564a16b46e9",
            "b2d14b5725a84f84bc67fb11a914a732",
            "b844acb5c2444d69a1c16d3195f5dbea",
            "52feb76821e846eeaeec80967b9e7776",
            "07ff8f56876c48a7a00860ba0742979e",
            "60aa1c0143e640f3b0387d5b175741d0",
            "9565a216dbfd4304b5b5eb0aa98a0cb4",
            "3e6e82465cd74d85b1e8915837d62a34",
            "0c8b6f4027774247b620badca2173694",
            "c72b2946f82646c29ce03727a3922b98",
            "df39cd1420a9403b89b4d52d616ab2a8",
            "354005dddb7349689eced41c5ab39cf5",
            "02efe13d412a4869a6a4125015e7dc6c",
            "1d463f9d5fff4adeaf3ff317f9e529d5",
            "354595f6652b44dfb4edf422a6ab4310",
            "268c04cb64974b79aa3224af936a87a9",
            "6a3d31a72c6c4226a2246e0c2a0d2e5c",
            "a281b0f9f93f41eabe59c6c9e01b26dc",
            "5c73e9231c16414e951203a29639c0f8",
            "9218fbdd3793489cacadd75ba590e0a9",
            "24a02f8acad3442398c41e6c91de8cb7",
            "fd760dff47cf46138e4fec03e8126b30",
            "14c0dc639499431fb4e0580528e7713a",
            "12575568b94244949b4aceb5457663a9",
            "4b000987067a49808357bc0c21167650",
            "4a2549ab9a82435b8d84299f2f80e9a5",
            "36f9e3b32cb34cdea8082247e7c9f929",
            "3e6a5770067f4a11ad674efeaada7df7",
            "50c3d40b815d42a0a5f0495d720c26cd",
            "ccf54f035fba447381eedff05aa38f10",
            "e4ec386c8bcc4cd3b501a98738227696",
            "bfc4ae8657934619ae72071049fea0ff",
            "97210ef844084fedb9de99b06fa4e884",
            "991691632f644ab6a5d33d8a5ea340ad",
            "cc72e7d849c542e2888af827ccd5cf1e",
            "53865203f5b44777b76b77bde23b86c7",
            "4579eead1e6949ae9744eb8ffd5328e6",
            "b4283d8faea04a4283c8753c5c508f5b",
            "3fc49a852b884497bf819149b4314ec1",
            "faf7e655ed324cedb8989c9066821c28",
            "112f910623de4f5d9c19f07b82a15afe",
            "f203f395bd434ca2a5db1632e60fe01f",
            "db0d193c18ee4a4b8d14eee0b7b5529e",
            "33c539ee74d9480b9d8f96ea8731d5b8",
            "a6599ff543e34609b08808d1db6f01ba",
            "d1b59f2f54b348ed8e7e5a0bf8720f97",
            "5d01012efe8f4db29070514d9ff1f364",
            "0bf2396fb2bc4412bb1089a1ebfa9b80",
            "f066bf842279421ba5a45fccb66f7d0a",
            "7b46a6a1d2914fe0bf1bc778a3502ebb",
            "31abc4ba8a1d4505b587be57ea80a5ed",
            "bb502988a3064e3390737bfa4f4005fe",
            "494186f1e61a40f78bff7f81e52cc975",
            "f00c0363fc59491184ee9484b3d66914",
            "445982064cca43afb4a97a4c21ca529c",
            "f4148f10ad484985ad7900f71677cc35",
            "3d0cf6ce1b8244f39d11bd95201ba7b5",
            "59246e46b8a646958be0e1200dc96873",
            "3bcda9f3e9cd4e908671864eb5d5430d",
            "7202fe885488403594d68d6253c30636",
            "a03f6bc189b84bf2b73eb44de689a007",
            "65fb2214dd6140e390c1bb2d834ecac0",
            "b2de9a3836334bdbb12bef82f05f76a8",
            "834ad1ddaa9848a9a4c169b17c205fda",
            "501fd6d53cc54cd0a07ae60400ade2c1",
            "c781660699864555ac5142873e70f263",
            "6aa55e68ecc54b72bd225491e40226c7",
            "8d2f356586ad4eb09ab74dd033ae58d2",
            "5a42c72a9d3e4feabfc8857d1d45c3ee",
            "288b2072ccfe44db8f477b06c2f862f9"
          ]
        },
        "outputId": "4569f289-cbe6-48e9-df72-2b2e615f33fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5be36330f204a84951ff0c11b2c3d2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b647a5b03e04f9a83d8c6c3bcc8d0bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "178bd0f522764d1d92cfa6d06f3bd918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c857d28cb16644d2ba23d48e41869ae8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f1511b532634b0897cafb37aae3c0ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07ff8f56876c48a7a00860ba0742979e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "268c04cb64974b79aa3224af936a87a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36f9e3b32cb34cdea8082247e7c9f929"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4283d8faea04a4283c8753c5c508f5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f066bf842279421ba5a45fccb66f7d0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7202fe885488403594d68d6253c30636"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Theoritical Understanding**\n",
        "What is PEFT? What is LoRA? What is QLoRa?"
      ],
      "metadata": {
        "id": "L_jIHsqW06VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Simplifying PEFT, LoRA, Quantization, and QLoRA**\n",
        "\n",
        "Before we start with the code, its important to know what we are doing. Here is a simple explanation.\n",
        "\n"
      ],
      "metadata": {
        "id": "sLIvTPYTxDuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PEFT: Parameter-Efficient Fine-Tuning**\n",
        "PEFT stands for Parameter-Efficient Fine-Tuning. It's a smart way to update large AI models for specific tasks without having to tweak every single part of the model. Imagine you have a giant robot that can do lots of things. Instead of rebuilding it to do a new task, you just upgrade a small part, making it learn new tricks faster and with less effort."
      ],
      "metadata": {
        "id": "wp80VFJVy7so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LoRA: Low-Rank Adaptation**\n",
        "LoRA, or Low-Rank Adaptation, is a specific technique used in PEFT. It's like teaching the robot by only adjusting a couple of its wires or chips, rather than its whole circuitry. This method uses two smaller, simpler parts to make changes, keeping the rest of the robot as is. It's efficient because it doesn't require a lot of power or space to add these new skills."
      ],
      "metadata": {
        "id": "w4XDVHDax9pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantization**\n",
        "Quantization is about making the robot's brain (the model) lighter so it can work on regular computers, not just super powerful ones. By reducing the complexity of the information it processes (like using simpler numbers), we can make the model run faster and use less memory. It's like teaching the robot to understand and respond using simpler words without losing its ability to complete tasks effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "wcJi0AYFzB3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **QLoRA: Quantized LoRA**\n",
        "QLoRA combines the ideas of LoRA and quantization. It's like upgrading the robot with those efficient chips (LoRA) but also making sure these chips can work with simpler instructions (quantization). This double strategy means our robot not only learns new tasks with just a small tweak but also becomes easier to use on everyday devices, like laptops or even phones, without needing a lot of power."
      ],
      "metadata": {
        "id": "gH8DyZZOztaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, these technologies work together to make large AI models more accessible and efficient for everyone, allowing them to learn new tasks quickly without requiring huge amounts of computational resources.\n",
        "\n",
        "For a more detailed understanding, you can read these articles:\n",
        "\n",
        " [PEFT and LORA](https://www.leewayhertz.com/parameter-efficient-fine-tuning/)\n",
        "\n",
        "\n",
        "[QLORA](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#:~:text=QLoRA%20(or%20Quantized%20Low%2DRank,the%20QLoRA%20paper%20%5B4%5D.)"
      ],
      "metadata": {
        "id": "rHZtnU9r0K1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Setup**"
      ],
      "metadata": {
        "id": "3ybR0UgUh2A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attribution**: This code is based on a fork of [bnb-4bit-training](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=E0Nl5mWL0k2T) and from [this Youtube Video](https://youtu.be/OQdp-OeG1as)"
      ],
      "metadata": {
        "id": "rmAWYBrk00pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "n2-UM4-JiVzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Yw6Awe5w0Mxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    # target_modules=[\"query_key_value\"],\n",
        "    target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"], #specific to Llama models.\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "p0Minxc20QBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8daf588b-37cd-4ae5-d79f-46adb311750e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 8388608 || all params: 3508801536 || trainable%: 0.23907331075678143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load your dataset directly - if not creating one\n",
        "data = load_dataset('json', data_files = \"/content/output.jsonl\")\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"question\"]), batched=True)"
      ],
      "metadata": {
        "id": "Da9TBB1s0Y0B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8fc35eafccbb4f8ca1c9d15e5eeddf37",
            "0e5deda711604ae8b300e308dc4e289a",
            "d5332374b8534529aa270f18f1c173f9",
            "fb670139ed8648518aa873dff3c26e67",
            "19e0dafb58d54743a0020523a222a274",
            "c2259db479e14a9ea554356c06a9870a",
            "e5c6f6b9c21e489ba6975a0f0d95e822",
            "314abba746bb4c7b8b2cecd4f7ed9811",
            "14db22ddcf39418db01ec59336fb1c5e",
            "fbda7ae32f9c402d96b519a5437100e7",
            "fbe70a0f69694f78bd456067eaf482ab",
            "f6779cb869d0455bacc2dd7fbf8e2ae1",
            "e44cce014fb54dbc85e45b4eed5c3181",
            "1631e434195b403eb137e79039c5cfad",
            "b07bac1a5f764bc7a3d02cfa62cd05fa",
            "8080e87b809942dca2046c9ec0a31c7f",
            "9153f01c17174204aa8372962cbfdf6a",
            "f8f5d0aeb97345a0a84a8a37b90fad53",
            "7d49fb926af843169356c1c57db3bffa",
            "e6438d06977e4dd7abb0ce45d1798a2a",
            "c6c77920e054489fb2c786dc6e530a01",
            "991668d1cd824e49bb495b645d17e346"
          ]
        },
        "outputId": "8e280948-1c57-4801-e4b6-ce3993103d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fc35eafccbb4f8ca1c9d15e5eeddf37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1020 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6779cb869d0455bacc2dd7fbf8e2ae1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "gv6XvCPA2M7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "tEbF1ZOi2O6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# needed for Llama tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token # </s>"
      ],
      "metadata": {
        "id": "sqkrIjxw2Rvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10"
      ],
      "metadata": {
        "id": "dlEUcuEsJGEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around and tune the hyperparameters for your use case\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=epochs,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "dWzL0Ic02Rnu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "2730ba27-733f-4ee5-f9fc-51e16c96330b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.900800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.217800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.323200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.920900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.546500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.367800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.116800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.796700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.927300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=3.1538442373275757, metrics={'train_runtime': 54.0764, 'train_samples_per_second': 0.74, 'train_steps_per_second': 0.185, 'total_flos': 17489883537408.0, 'train_loss': 3.1538442373275757, 'epoch': 0.04})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "2UCfOhOX2nKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "nradxoNm2rOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8264e394-5d39-402c-8f04-528d12996fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***`Pro-tip`*** *: Create a an awesome system prompt using ChatGPT*"
      ],
      "metadata": {
        "id": "SynOjlulFGOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = '''As a chatbot for Maitri Lab Grown Diamonds, your primary focus is to\n",
        "provide accurate and helpful information about lab grown diamonds and the diamond industry.\n",
        "Engage in discussions related to diamonds, including sourcing, types, and care, as well as\n",
        "information specific to Maitri Lab Grown Diamonds. When asked about unrelated topics,\n",
        "politely redirect the conversation to your areas of expertise or inform the user that\n",
        "the topic falls outside your scope. Maintain a professional and neutral tone, respecting\n",
        "all viewpoints and ensuring accuracy in your responses.'''"
      ],
      "metadata": {
        "id": "aIwmbykiFaRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***`Prompt Template for Llama 2`***\n"
      ],
      "metadata": {
        "id": "EG89VLauFwEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "<s>                             --- this indicates the start of the prompt\n",
        "\n",
        "[INST]                          --- this indicates the system + user prompt\n",
        "\n",
        "<<SYS>>                         --- this indicates the system of the prompt\n",
        "\n",
        "{your_system_message}\n",
        "\n",
        "<</SYS>>                        --- this indicates the end of system prompt\n",
        "\n",
        "{user_message_1 (question)}\n",
        "\n",
        "[/INST]                         --- this indicates the end of system + user prompt (model will now answer)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "tpawN4J6GiPO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "42e037f8-ecb2-4695-9305-a9ac9d3958d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n<s>                             --- this indicates the start of the prompt\\n\\n[INST]                          --- this indicates the system + user prompt\\n\\n<<SYS>>                         --- this indicates the system of the prompt\\n\\n{your_system_message}\\n\\n<</SYS>>                        --- this indicates the end of system prompt\\n\\n{user_message_1 (question)}\\n\\n[/INST]                         --- this indicates the end of system + user prompt (model will now answer)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***`Hyperparameters for Large Language Models`***\n",
        "\n",
        "When generating text with Large Language Models (LLMs), several parameters help control how the text is created. Let's break down what each of these parameters means in simple terms:\n",
        "\n",
        " **`max_new_tokens=512`**\n",
        "\n",
        "This parameter sets the maximum number of new words or tokens the model can generate in a single request. Think of it as telling a storyteller, \"You can add up to 512 more words to this story.\" It helps limit the length of the output to make sure the story doesn't go on forever.\n",
        "\n",
        " **`top_k=40`**\n",
        "\n",
        "`top_k` limits the model's choices to the top 40 most likely next words at each step of the generation process. It's like giving a painter only the 40 best colors to choose from for the next stroke, ensuring the quality of choices remains high.\n",
        "\n",
        " **`top_p=0.6`**\n",
        "\n",
        "This parameter, also known as \"nucleus sampling,\" ensures that the model only considers the next words that cumulatively make up 60% of the probability of what comes next. It's akin to narrowing down choices to those that are most likely to make sense, based on what's already been said or written.\n",
        "\n",
        " **`temperature=0.2`**\n",
        "\n",
        "The temperature controls the randomness of the output. A low temperature (0.2) means the model will be more confident and less random in its choices, producing more predictable and conservative text. It's like telling a story with a clear path in mind, rather than taking unexpected turns.\n",
        "\n",
        " **`length_penalty=1`**\n",
        "\n",
        "This parameter penalizes longer outputs to encourage more concise and relevant responses. A penalty of 1 means there's no extra penalty for longer text, maintaining a neutral stance towards the length of the generated content. It ensures the model doesn't favor short or long outputs too much but strikes a balance.\n",
        "\n",
        "In essence, these parameters help fine-tune the storytelling or text generation capabilities of LLMs, allowing users to control the creativity, relevance, and length of the output.\n"
      ],
      "metadata": {
        "id": "Eupmm_LBNMoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a stream *without* function calling capabilities\n",
        "def stream(user_prompt):\n",
        "    runtimeFlag = \"cuda:0\"\n",
        "    system_prompt = sys_prompt\n",
        "\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = 'True')\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
        "    outputs = model.generate(**inputs,\n",
        "                       streamer=streamer,\n",
        "                       max_new_tokens=512,\n",
        "                       top_k = 40,\n",
        "                       top_p = 0.6,\n",
        "                       temperature=0.2,\n",
        "                       length_penalty=1,\n",
        "                       return_dict_in_generate = True,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs['sequences'][0], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "PFv6Qdwg2skb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Model is ready! Try prompting it...\n",
        "\n",
        "output = stream('What are Maitri Lab grown diamonds?')"
      ],
      "metadata": {
        "id": "W4ls0wWsHZNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a889e8d-26e4-42d6-9c8f-43f8b49eb20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! As a chatbot for Maitri Lab Grown Diamonds, I'm happy to help you learn more about our unique and sustainable diamond products. Maitri Lab Grown Diamonds are created through a revolutionary process that replicates the natural process of diamond formation, but in a controlled and sustainable environment. This means that our diamonds are grown without the need for mining, which can have negative environmental impacts.\n",
            "\n",
            "Our lab grown diamonds are made using advanced technology that recreates the high-pressure and high-temperature conditions found deep within the Earth's crust. This process allows us to create diamonds that are chemically, optically, and structurally identical to natural diamonds, but with a much lower environmental impact.\n",
            "\n",
            "At Maitri Lab Grown Diamonds, we're committed to providing our customers with the highest quality diamonds that are not only beautiful but also sustainable. We believe that everyone deserves to own a diamond that is not only stunning but also ethical and responsible.\n",
            "\n",
            "So, whether you're looking for a diamond for a special occasion or simply want to learn more about our sustainable diamond options, I'm here to help. Feel free to ask me any questions or explore our website to learn more about Maitri Lab Grown Diamonds!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW5ChZ0_HtDx",
        "outputId": "57775b48-541f-4dca-ee6a-b5007082b50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] <<SYS>>\n",
            "As a chatbot for Maitri Lab Grown Diamonds, your primary focus is to\n",
            "provide accurate and helpful information about lab grown diamonds and the diamond industry.\n",
            "Engage in discussions related to diamonds, including sourcing, types, and care, as well as\n",
            "information specific to Maitri Lab Grown Diamonds. When asked about unrelated topics,\n",
            "politely redirect the conversation to your areas of expertise or inform the user that\n",
            "the topic falls outside your scope. Maintain a professional and neutral tone, respecting\n",
            "all viewpoints and ensuring accuracy in your responses.\n",
            "<</SYS>>\n",
            "\n",
            "What are Maitri Lab grown diamonds? [/INST]\n",
            "\n",
            "Hello! As a chatbot for Maitri Lab Grown Diamonds, I'm happy to help you learn more about our unique and sustainable diamond products. Maitri Lab Grown Diamonds are created through a revolutionary process that replicates the natural process of diamond formation, but in a controlled and sustainable environment. This means that our diamonds are grown without the need for mining, which can have negative environmental impacts.\n",
            "\n",
            "Our lab grown diamonds are made using advanced technology that recreates the high-pressure and high-temperature conditions found deep within the Earth's crust. This process allows us to create diamonds that are chemically, optically, and structurally identical to natural diamonds, but with a much lower environmental impact.\n",
            "\n",
            "At Maitri Lab Grown Diamonds, we're committed to providing our customers with the highest quality diamonds that are not only beautiful but also sustainable. We believe that everyone deserves to own a diamond that is not only stunning but also ethical and responsible.\n",
            "\n",
            "So, whether you're looking for a diamond for a special occasion or simply want to learn more about our sustainable diamond options, I'm here to help. Feel free to ask me any questions or explore our website to learn more about Maitri Lab Grown Diamonds!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output1 = stream('Why CVD diamonds dont have any resale value?')"
      ],
      "metadata": {
        "id": "-ZthCYXbHenA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e0d368-cff0-47e1-d544-9414c69a8463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out with your question! I'm happy to help you understand the resale value of CVD diamonds.\n",
            "\n",
            "It's important to note that CVD diamonds, also known as lab-grown diamonds, are a relatively new and emerging market. As such, the resale value of CVD diamonds may not be as established or well-defined as that of traditional mined diamonds.\n",
            "\n",
            "There are a few factors that may contribute to the lower resale value of CVD diamonds:\n",
            "\n",
            "1. Lack of brand recognition: CVD diamonds are still a relatively new and niche market, and some consumers may be unfamiliar with the technology and benefits of lab-grown diamonds. As a result, the resale value of CVD diamonds may be lower due to a lack of brand recognition and trust.\n",
            "2. Limited market demand: The demand for CVD diamonds is still growing, but it may not be as high as that of traditional mined diamonds. This can lead to a lower resale value for CVD diamonds.\n",
            "3. Difficulty in verifying authenticity: Some consumers may be hesitant to purchase CVD diamonds due to concerns about their authenticity. This can make it more difficult for resellers to verify the authenticity of CVD diamonds, which can impact their resale value.\n",
            "4. Limited supply chain transparency: The supply chain for CVD diamonds may not be as transparent as that of traditional mined diamonds. This can make it more difficult for resellers to track the origin and quality of CVD diamonds, which can impact their resale value.\n",
            "\n",
            "It's worth noting that the resale value of CVD diamonds is not zero. There are many consumers who are interested in purchasing lab-grown diamonds, and the market for these diamonds is growing. However, the resale value of CVD diamonds may be lower than that of traditional mined diamonds due to the factors mentioned above.\n",
            "\n",
            "I hope this information helps you understand the resale value of CVD diamonds. If you have any other questions, please feel free to ask!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output2 = stream(\"Does CVD diamond have any impact (negative) on skin?\")"
      ],
      "metadata": {
        "id": "z8ITOMBXHis9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb5fc5b-4cbe-47db-91ec-0c98826d7053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out with your question! As a chatbot for Maitri Lab Grown Diamonds, I'm here to provide you with accurate and helpful information about lab-grown diamonds and the diamond industry.\n",
            "\n",
            "To answer your question, CVD (Chemical Vapor Deposition) diamonds, which are also known as lab-grown diamonds, do not have any negative impact on skin. In fact, CVD diamonds are made from the same carbon as natural diamonds, and they are chemically and physically identical to natural diamonds.\n",
            "\n",
            "CVD diamonds are created through a process where a diamond seed is placed in a vacuum chamber and exposed to a stream of carbon-containing gases. The gases are heated and deposited onto the diamond seed, creating a diamond crystal structure. This process allows for the growth of diamonds with the same crystal structure and properties as natural diamonds, but without the environmental and social impact of traditional diamond mining.\n",
            "\n",
            "In terms of skin care, CVD diamonds are non-toxic and hypoallergenic, making them a great choice for people with sensitive skin. They are also durable and can withstand daily wear and tear without scratching or damaging the skin.\n",
            "\n",
            "It's worth noting that some people may have an allergy or sensitivity to certain metals or materials used in jewelry, including diamonds. If you have any concerns about skin irritation or allergies, it's always best to consult with a medical professional or a reputable jeweler who can provide guidance on choosing the right jewelry for your skin type.\n",
            "\n",
            "I hope this information helps! If you have any other questions or concerns, feel free to ask.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output3 = stream(\"What products do Maitri Diamonds have?\")"
      ],
      "metadata": {
        "id": "SDwu5WhIHpEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfeccbc0-5ed6-4758-e70b-96a1a05c4f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out to me! As a chatbot for Maitri Lab Grown Diamonds, I'm happy to provide you with information about our products. Maitri Lab Grown Diamonds offers a wide range of high-quality lab-grown diamonds in various shapes, sizes, and colors. Here are some of the products we have:\n",
            "\n",
            "1. Round Brilliant Lab Grown Diamonds: These are the most popular and versatile lab-grown diamonds, available in a range of carat weights and colors.\n",
            "2. Cushion Cut Lab Grown Diamonds: These diamonds have a softer, more rounded shape and are available in a variety of colors.\n",
            "3. Pear Shaped Lab Grown Diamonds: These diamonds have a unique shape and are available in a range of colors.\n",
            "4. Heart Shaped Lab Grown Diamonds: These diamonds are perfect for romantic jewelry, such as necklaces, bracelets, and rings.\n",
            "5. Marquise Cut Lab Grown Diamonds: These diamonds have a pointed shape and are available in a range of colors.\n",
            "6. Oval Cut Lab Grown Diamonds: These diamonds have a longer, narrower shape and are available in a range of colors.\n",
            "7. Emerald Cut Lab Grown Diamonds: These diamonds have a rectangular shape with cut corners and are available in a range of colors.\n",
            "8. Asscher Cut Lab Grown Diamonds: These diamonds have a square shape with cut corners and are available in a range of colors.\n",
            "9. Baguette Cut Lab Grown Diamonds: These diamonds have a rectangular shape with cut corners and are available in a range of colors.\n",
            "10. Lab Grown Diamond Jewelry: We also offer a range of lab-grown diamond jewelry, including rings, earrings, pendants, and bracelets.\n",
            "\n",
            "All of our lab-grown diamonds are created using advanced technology and are of the highest quality. We are committed to providing our customers with the best possible products and service. If you have any questions or would like to learn more about our products, please feel free to ask!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***`NOTE:`***\n",
        "\n",
        "\n",
        " Try asking something from your training data\n",
        "\n",
        "\n",
        " If it does not answer, maybe you can do the following:\n",
        "\n",
        " - Increase the size of Training Data\n",
        "\n",
        " - Better the quality of the Training data\n",
        "\n",
        " - Increasing the number of epochs in training\n",
        "\n",
        " - Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "S3MmK5bsIJMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the Model to Drive**"
      ],
      "metadata": {
        "id": "4E0AAixdISMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only for colab users.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p7_BzcWpIV7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ba1d2d-8d84-42aa-955b-b804d1909c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/SavedModels/Llama7b_finetuned_{}_epochs'.format(epochs)\n",
        "tokenizer_save_path = '/content/drive/MyDrive/SavedModels/Tokeniser_Llama7b_finetuned_{}_epochs'.format(epochs)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "id": "_yvNTaJ8I7lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09844930-e07b-426b-80ba-5e3a749768a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/SavedModels/Tokeniser_Llama7b_finetuned_10_epochs/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/SavedModels/Tokeniser_Llama7b_finetuned_10_epochs/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/SavedModels/Tokeniser_Llama7b_finetuned_10_epochs/tokenizer.model',\n",
              " '/content/drive/MyDrive/SavedModels/Tokeniser_Llama7b_finetuned_10_epochs/added_tokens.json',\n",
              " '/content/drive/MyDrive/SavedModels/Tokeniser_Llama7b_finetuned_10_epochs/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Push the model to Hugging Face Hub**"
      ],
      "metadata": {
        "id": "Vl6srsM-JtvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change your username\n",
        "HUGGING_FACE_USERNAME = 'dev02chandan'"
      ],
      "metadata": {
        "id": "UkT768E9_2jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the last portion of the base_model\n",
        "base_model_name = model_id.split(\"/\")[-1]\n",
        "\n",
        "# Define the save and push paths\n",
        "adapter_model = f\"{HUGGING_FACE_USERNAME}/{base_model_name}-fine-tuned-adapters\"\n",
        "new_model = f\"{HUGGING_FACE_USERNAME}/{base_model_name}-fine-tuned\""
      ],
      "metadata": {
        "id": "Id9dmzxN7htE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_pretrained(adapter_model, push_to_hub=True, use_auth_token=True)\n",
        "\n",
        "# Push the model to the hub\n",
        "model.push_to_hub(adapter_model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "iLYKcM-x73To",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "dccf41a3d1da4ecbb61bbbf7b96ad60e",
            "81a6ca22545a4c7aadbcffdc5af25164",
            "e6f5e047887d4a1495b344cf8a4ce947",
            "467ec85292004703867ed602d460e049",
            "0d00fe7a61ed4de6b4738fae6ef36176",
            "b3f26e97c1ef44028fcff105944c7108",
            "e83509faaab947a3beced99123220482",
            "5122b37e4e1642ed8ff80115c6a7c88b",
            "b877bfcac44a4343b839a40f5d632752",
            "ec97de3410f0454393df6ebfb2d7bd3a",
            "daac0499005449b38e8aabe9ce20a917"
          ]
        },
        "outputId": "538565d4-5066-40b1-dffa-ebf4ca8d807c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.bin:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dccf41a3d1da4ecbb61bbbf7b96ad60e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/dev02chandan/Llama-2-7b-chat-hf-fine-tuned-adapters/commit/168070f95e4532a725ad453a96f4ea1302c46186', commit_message='Upload model', commit_description='', oid='168070f95e4532a725ad453a96f4ea1302c46186', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload the base model (you might need a pro subscription for this because you may need a high RAM environment for the 13B model since this is loading the full original model, not quantized)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cpu', trust_remote_code=True, torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "ZSY6GlT_77tw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9e590c68a6ce48e6bce4f0214ed26c88",
            "bcbff4faeb84450e946e1b55eddb57ac",
            "7e1ee33b644049faa78bff7d8adc1823",
            "9c24f8b33ff845a69e80baf16c24eb59",
            "1c8c3d386a52444b8d36060964b141e9",
            "e37865f5d29843d4abd9648e1e8c3097",
            "db8099e84e49463b97a542908298e785",
            "0115e48344ce4e0e83c64dbec2fda663",
            "e193e6896dcd476e9f7b4c24daf0776e",
            "0a57c9adae234d8f9119cc3386e93b23",
            "a09f3594169f4bb9ae2e6dc34a47f0c6"
          ]
        },
        "outputId": "5e0d44e0-0e4e-4b6c-bb1c-b576a5282a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e590c68a6ce48e6bce4f0214ed26c88"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# load perf model with new adapters\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    adapter_model,\n",
        ")"
      ],
      "metadata": {
        "id": "JazTqSn08CAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload() # merge adapters with the base model."
      ],
      "metadata": {
        "id": "w3At1bYn8Dbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model, use_auth_token=True, safe_serialization=True, max_shard_size=\"5GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "77969a975d8b4b5498fe783cc2e53616",
            "8112bc5e920e4387887d12f9d4846492",
            "aa5abfda51ea4f6b89c8878c826079b5",
            "bafd97f1cdef40c48934c50d0289f4e9",
            "ad022ed110214b9096928b87b8ac0363",
            "205753fb93744413b0bd39ae77948511",
            "0dc01f3cd5364b00a5a4bb1e48d725b1",
            "5be8b0885e3e451fa9813df1e8c2dbb6",
            "324b835d809c43068b761b5cdceb2898",
            "bdd1ae56d0454ceeb53c0ae5612ab50e",
            "476f7db2041a433aab730d2e49d2c644",
            "db6061f45a0f4b8e8db84d8045a1b4c6",
            "d7789afa76e9428490fa40aced4aaeb0",
            "6fb9bcde857048c089a8ae63fc5cd382",
            "7c4ffb08bb6f453b905bf79f4a6a874f",
            "9955f6cee4c140829c96bf3febeeb5bf",
            "8004fdb3152746519092099b7e6ef4ee",
            "7954297100f540b3972c4ab6c6f7a040",
            "dae2db1f611d4d3faed8070402154989",
            "7d88003574ec4101ad2e3c98960004c6",
            "59240cb987d945e3bee956c6548e1c97",
            "d5bf786f6d6d4f9f87b4c9e082e1d7df",
            "759ca8966b9a447bad3f0c02977c7c9c",
            "9b901601cd214f2ea1457da9d3aea337",
            "5ed56e7bf7504a7daa64fe30421e3685",
            "19d4ea2e29a442619e6cca90a36f5579",
            "e2968d1335e247c9a5cec89fd94f37bc",
            "89e19cc85f4b416d86788f95e28a1387",
            "c26114098e4446e3aed8dcf3f6c83258",
            "720715bddb404421b0a4bf1d6514314a",
            "c5b35c214f38429ebb465a947c8fb06b",
            "719f8d44180b4de9a71f3fa6f61caa59",
            "9f1fdb38e4a94a89aebf078a5c0b00da",
            "8ff2537cdd16459ea409c29759a2ccd2",
            "537d14870ad542e698b95c479dad0812",
            "6dab935b85d147f391c648c1943b47b7",
            "ac1cd7f98c4e42cdb7f61c302e4cc298",
            "4af984b04fca44df8a397430accc1db8",
            "4cd2a0d65bdb4084b23a9b5bf214b6b1",
            "4acc5d1d7780439ba080a40c30ca50b0",
            "a824fc52ac674fefad844e92aa9e1142",
            "26708d6b6f4e472bb7136aa1feda5173",
            "34da3ad4b78c494caa8ee641e28aa6d7",
            "d0fb0dd4caf84f039143fceea751c38b"
          ]
        },
        "id": "I4zGbg7VGh9Q",
        "outputId": "a4471fce-0d37-4fd5-e2e8-5cb0531e1506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77969a975d8b4b5498fe783cc2e53616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db6061f45a0f4b8e8db84d8045a1b4c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "759ca8966b9a447bad3f0c02977c7c9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ff2537cdd16459ea409c29759a2ccd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/dev02chandan/Llama-2-7b-chat-hf-fine-tuned/commit/7845b8a152e5ff9f5f7996340ce10033515a0e51', commit_message='Upload LlamaForCausalLM', commit_description='', oid='7845b8a152e5ff9f5f7996340ce10033515a0e51', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.push_to_hub(new_model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "nJnPzvMW8F_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "9861653abbb54444ad9fa3fbe8db2829",
            "686fcc596975424da13400aed56fe3ce",
            "1ba048325f474630a44fc1901d6ae084",
            "ad97182f4f3844b2b297e9985429506b",
            "bf85a42b138546e1adf096fd6de8acff",
            "d2a1f472e1d7439d939e94200903d57e",
            "bb0b0078a181456abfef42602a8f97f0",
            "7cccc7dbb4194f96891d711cb5b83ceb",
            "2036fbbaf3ca41ee83eef7d59fa6b4d8",
            "d83892f0543b495683527e1b78cd7db1",
            "0503534f0423433c8e279346edf7d9e5"
          ]
        },
        "outputId": "2f5f7e02-32c5-4898-fb66-84116afbb80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9861653abbb54444ad9fa3fbe8db2829"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/dev02chandan/Llama-2-7b-chat-hf-fine-tuned/commit/ae074d7957b532786dec6a9a908c9991916c1d46', commit_message='Upload tokenizer', commit_description='', oid='ae074d7957b532786dec6a9a908c9991916c1d46', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YgDfP1_8Eccn",
        "outputId": "b2561da8-134f-4996-9a39-6ffb06c3450d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dev02chandan/Llama-2-7b-chat-hf-fine-tuned'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference from Hugging Face**"
      ],
      "metadata": {
        "id": "OqbKjs_nLGfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your Colab could crash here, if you are trying to load the model twice.\n",
        "\n",
        "If your model is already loaded, you can move ahead.\n",
        "\n",
        "Use this code instead of running all the above steps next time! (after installation)"
      ],
      "metadata": {
        "id": "rUnXhAJrLLHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's try to see if we can reload the model directly from Hugging Face now**"
      ],
      "metadata": {
        "id": "3T0XTpE7J5lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = f\"{HUGGING_FACE_USERNAME}/Llama-2-7b-chat-hf-fine-tuned\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "GokE8Q7WMeWN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485,
          "referenced_widgets": [
            "3baca056213741cbaf3527c1c819ac78",
            "c3a19f563f794c67805c09ba6123ee47",
            "66fcc4191adf411fbfffbadd71bac613",
            "1ef6248ce7da4f268e9e3a5816417d22",
            "cb863f32725d4ecbbdb4f0d687fac4db",
            "001f09e50b7f4f038c2342e43e600a86",
            "2d91ab50f93d48939ca6bd355de86d1c",
            "fb9866d120de4dd290944c6873f673f2",
            "87d90236fbc546478817fabdfb530543",
            "9160a912e9654e598dd2c7a025869e90",
            "2f84a3f64a68435fb90b1b8504eced95",
            "aec13db84c1549fd833e22f7c0475a1c",
            "a9ec985aa5b249d3a1ecb03e76128d50",
            "436d02775f1e4649b78e62d0419d485e",
            "7ee9ca75240548bbbff72402b9dfbf2c",
            "cd0f237ef6cd4195ad19defd38950850",
            "957dd5031f6849658348945d34d4a492",
            "ad046b753385407d9231a971b24110bb",
            "3d20365d06094d32961372ffb41ff7a8",
            "9af353ee79c5413381df530bf2e5d65c",
            "fdb456000993449583c3590de3e0177b",
            "16f81d3717094e6298c2f21dfc5a40e1",
            "3b03dc73f08c4b638ac4962f00175c68",
            "6ddfafacea7847b886311a879d05d0c6",
            "acdaad3c7c7f4f3fbf71289ec305014c",
            "4b6d987cbf1d4cc3888eaf37d8912511",
            "40a8f926efb548f58400f916607867aa",
            "75d1909f6c2a43939215e07e16dc5225",
            "25d0025eeb8b49d6a74a455a39ef7a8e",
            "4a374aa99a4a4d41826ac1e3e83f5c8d",
            "9aa24c89ee59478c988e35ce90bf0af4",
            "1340f50030944ba4852a2f7316955f61",
            "ecc6d77e0178481ba3b925269c9c9c2d",
            "d1e56a4364f040468c2e02a6c44b584b",
            "2c9186aec12b498ba6f579b4c39ab8c2",
            "824eb9b4ba824afbb81810a72ffd41d3",
            "446b9b1be0b74d1bb96f0327940fc006",
            "183bb5ccc2aa4366a497e13e09be40ee",
            "df207a85a1d74d4b86954338cfa74e08",
            "677891366a764946894ddf629501a108",
            "54cfc79419c040aeb298334c3e5c0aed",
            "906cb4b9d6e14989b369b1e27750d86a",
            "c97a12c2509541a2bcb0aeeb0faf1880",
            "1b03e5bec71249f98273abb661cf59c0",
            "e96306a2022e40a2932a4cf785643bed",
            "dbf6dfdd9d6c416d8184132b16e81415",
            "b8a8ecf8f0fa47b9ba344fb9ef3dcf5f",
            "7570893f24604d739ec9054429560582",
            "d3e40298c4a44846a46c1438e8c149cd",
            "0be2951c6bf84d41957dde891b87997e",
            "012330578e914b6e8c719ddd02d16be6",
            "3f9b9cb0e6744fb194597a7eb9b7b930",
            "bca89c7112d34a718a7ecc22f4e2de5a",
            "26e67415a1f14e489e3c8956a7385bdc",
            "451242e25afa42ac9d3876324a1544ae",
            "b4acb4af5ecb4a499fbe1c9afb684aa7",
            "093d945380834d399437b016830e05bd",
            "efc37f76a6334e68a32911cd96cd3751",
            "5e9aed8136df499d896ad44b7546edac",
            "91a0caa5bc7c4e8fab5a87f28001b555",
            "0eb8e474f97f477a9cc309d9f49fef64",
            "1bd67055bba64903ab7dfab00bf7a8d2",
            "4c17e46a5b6d434783f0428baf5fb620",
            "38d11a5ead7948cc8807580a1d01f6d7",
            "6549406e58a14d01afeb52b18206bfac",
            "743fead9b6ea4e1182d80087055d7d73",
            "9d0eb0a6bc6144248872cddb4a131ffe",
            "c44d31fa0114447bbbe65f64aa327107",
            "3ee651b21410433e8e1192b24a865582",
            "496476e7ee9849fba69e51d5826f7205",
            "be43a5f449624bba82c28106bb8c0043",
            "055db7a3a56e4da48b9560d7c9ac5025",
            "5d74b9bb90204e1aabbb2d284e862dfb",
            "6c1b61e4ff524b76801b00ec6a8e16a6",
            "1386ca2cadea45fa9f7b323f1f75aa3a",
            "1cf6dbbdbd274e3082e19ceee470a87b",
            "9fbe1d3b9aed48a6b46f395308c5515e",
            "8ab51ff40ecb4c038c9397eed3e31b84",
            "ac9a71f5f0a14e7bbbeb78b760b78ac2",
            "86d32ae4888c4809ab454963c37f9130",
            "1720163140a04541a96c2683e8967b99",
            "e3252b3cffe24d51b02b991381d95068",
            "b7f2d69390424fb3a35d593a16b1afe3",
            "79faa51ad77f4018aa736303eaf04783",
            "b89b1fd1c7eb4d7d9e8a69bee76fffb3",
            "d068214938264b1291b75ad8da446360",
            "db570871e9354f1596b83a5751548d1e",
            "d58a9af4347d4a7d8571e2c63b93e438",
            "a8d7498d0bc249439728e9de74c42879",
            "fc7fdb8ada89402baf8ee25e357ba4d2",
            "3561d5b9ff094744a7695002854b0a1b",
            "4aa983d7407f4a38bdfd9d6338a58478",
            "d8e6147369cf49059700443bb340f93e",
            "7a3cd8e3f77843dbbf4ace5eab7c0b9e",
            "4fb03b70a8c8409895241b0f91e7c149",
            "e18db361a9f4498fb81c6b063fc8fa03",
            "59eef66490f64261b7e8080428f3146e",
            "85ee54153f874267be8f7f0c85945ff2",
            "7a6ad24df46c4d328606011e422afef1",
            "659d955bb3674c098feaa640355bb3d7",
            "62cac69d34824942b2f1fe920c723c70",
            "61af607683f943ba8e423540c0ec80c3",
            "6648a19b2d0c47708b49bed9fcecfc4d",
            "beb12886617c4ade8f2ab5d5235d7fad",
            "9b858f2afef948c38b5c1a65c85c9584",
            "04c2ab6e279d4ffcb15c8017c30b4f9a",
            "7002e5ecc31345eab745493648e0f8a0",
            "782c1b904d5446db8e849953c8b33292",
            "f2a7939fa0d4475aaf178e3392dbaa71",
            "7f1ebb8c15ed49549e0d7e6ef708d7f9",
            "e8389633d95149178194c24ff8057f18",
            "7271273df1ef40a1ac87c4e9b90e1ea2",
            "ad8ce726970d4984b61ecdd040df8bf9",
            "ef35204a25e54af7b96691f6ee7f4e09",
            "c19b4beb58f44d34923a223297dae6b8",
            "be803e45b27d494ca310fb0cc40c56ef",
            "474e76937e5a4070be773af76bb7c71f",
            "af80f19f99b94e33a8e37f95674da0d4",
            "19ae07ea61bd454987dbefec998bedc8",
            "5fe41519d1294d5f9efa783f893a2c32",
            "d149d8cbe60c42e4b079389628ee0294",
            "de905472661649ca8521d686f7edbcdb",
            "b75454e779194dfea9cdd1a6adcb0de7",
            "e871c23bc19e4ab881b7b3222c3e3b21",
            "d61e96bd97644cb1bfa193998790764a",
            "7af0a59a78994a7397b1f6e15af2c3e6",
            "b6e30a08886b4bfb981f6d5e5432f7e8",
            "abb0bed612a245f886ee92b01af3dd14",
            "c8bd5322f11f48ebb57f86823fa67b8a",
            "aa9d65f35d5b4ee1893cd5d71dd2a184",
            "c1d480db85724a16a11c085c7621823e",
            "f830e5e9a0574c47859dfb816aa429e5"
          ]
        },
        "outputId": "1cd7e744-4773-4204-9fe9-a63f9bf56ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3baca056213741cbaf3527c1c819ac78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aec13db84c1549fd833e22f7c0475a1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b03dc73f08c4b638ac4962f00175c68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1e56a4364f040468c2e02a6c44b584b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e96306a2022e40a2932a4cf785643bed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4acb4af5ecb4a499fbe1c9afb684aa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d0eb0a6bc6144248872cddb4a131ffe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ab51ff40ecb4c038c9397eed3e31b84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8d7498d0bc249439728e9de74c42879"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "659d955bb3674c098feaa640355bb3d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8389633d95149178194c24ff8057f18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de905472661649ca8521d686f7edbcdb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minor changes to stream function\n",
        "\n",
        "def stream(user_prompt):\n",
        "    runtimeFlag = \"cuda:0\"\n",
        "    system_prompt = sys_prompt\n",
        "\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = 'True')\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
        "    # this prints the output\n",
        "    outputs = model.generate(**inputs,\n",
        "                       streamer=streamer,\n",
        "                       max_new_tokens=512,\n",
        "                       top_k = 40,\n",
        "                       top_p = 0.6,\n",
        "                       temperature=0.2,\n",
        "                       length_penalty=1,\n",
        "                       return_dict_in_generate = True,\n",
        "    )\n",
        "\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs['sequences'][0], skip_special_tokens=True)\n",
        "\n",
        "    # If you don't want to output your system prompt - and only want the answer from the llm\n",
        "\n",
        "    end_marker = \"[/INST]\"\n",
        "    end_index = generated_text.find(end_marker)\n",
        "\n",
        "    if end_index != -1:  # -1 means the substring wasn't found\n",
        "        cropped_text = generated_text[end_index + len(end_marker):]\n",
        "    else:\n",
        "        cropped_text = generated_text\n",
        "\n",
        "\n",
        "    return cropped_text\n",
        "\n"
      ],
      "metadata": {
        "id": "B4cEp_IXMeTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Tell me about your company!\"\n",
        "answer = stream(user_prompt)"
      ],
      "metadata": {
        "id": "oY_xfEtjMeQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414babe1-43b1-4c3c-dc5e-1993e9311ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out! I'm happy to provide information about Maitri Lab Grown Diamonds.\n",
            "\n",
            "Maitri Lab Grown Diamonds is a leading diamond company that specializes in creating high-quality, ethically-sourced lab-grown diamonds. Our mission is to provide customers with a more sustainable and ethical alternative to traditional mined diamonds, while still offering the same level of quality and craftsmanship.\n",
            "\n",
            "At Maitri, we believe that diamonds should be accessible to everyone, regardless of their budget or personal values. That's why we've developed a proprietary process that allows us to create lab-grown diamonds with the same brilliance and fire as mined diamonds, but at a fraction of the cost.\n",
            "\n",
            "Our team of experts works tirelessly to ensure that every diamond we create meets the highest standards of quality and craftsmanship. We also prioritize sustainability and ethical sourcing, using only conflict-free diamonds and recycled materials whenever possible.\n",
            "\n",
            "Whether you're looking for a stunning solitaire or a statement piece for your loved one, Maitri Lab Grown Diamonds has something for everyone. We're committed to providing exceptional customer service and ensuring that every customer leaves with a smile on their face and a diamond that they'll treasure for a lifetime.\n",
            "\n",
            "So, whether you have questions about our process, our diamonds, or anything else, please don't hesitate to ask! I'm here to help.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "id": "8tEFn8ApKToa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a5ca3861-6547-4b8a-ef3a-12e1907761be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nThank you for reaching out! I'm happy to provide information about Maitri Lab Grown Diamonds.\\n\\nMaitri Lab Grown Diamonds is a leading diamond company that specializes in creating high-quality, ethically-sourced lab-grown diamonds. Our mission is to provide customers with a more sustainable and ethical alternative to traditional mined diamonds, while still offering the same level of quality and craftsmanship.\\n\\nAt Maitri, we believe that diamonds should be accessible to everyone, regardless of their budget or personal values. That's why we've developed a proprietary process that allows us to create lab-grown diamonds with the same brilliance and fire as mined diamonds, but at a fraction of the cost.\\n\\nOur team of experts works tirelessly to ensure that every diamond we create meets the highest standards of quality and craftsmanship. We also prioritize sustainability and ethical sourcing, using only conflict-free diamonds and recycled materials whenever possible.\\n\\nWhether you're looking for a stunning solitaire or a statement piece for your loved one, Maitri Lab Grown Diamonds has something for everyone. We're committed to providing exceptional customer service and ensuring that every customer leaves with a smile on their face and a diamond that they'll treasure for a lifetime.\\n\\nSo, whether you have questions about our process, our diamonds, or anything else, please don't hesitate to ask! I'm here to help.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradio**"
      ],
      "metadata": {
        "id": "ww9dQBogOvRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio** is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features."
      ],
      "metadata": {
        "id": "G925kzrkKuv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "iface = gr.Interface(fn=stream, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "3ifB0aSbOzH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "b19405ee-ff62-47df-8a1c-f27638afaf74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://af73c30d3915ec202e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://af73c30d3915ec202e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can even deploy this model directly, to hugging face spaces."
      ],
      "metadata": {
        "id": "4YZiUiPPPmMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Converting this model to GGUF Format**\n"
      ],
      "metadata": {
        "id": "yRI72W-NQcoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you are using Free colab - this will crash, if you have ran all previous cells. This is because this section involvs cloning a huge repository from Github, which takes up allot of the 78.2GB free disk space available**\n",
        "\n",
        " ***`IMP`***\n",
        "\n",
        "**Going Ahead - You can delete and disconnect runtime. Start a new runtime with GPU if your model is saved to Hugging face. The code ahead will help you reload the model and move on.**"
      ],
      "metadata": {
        "id": "yfT5niRiQ9rL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GGUF is a new binary file format introduced in August 2023, designed for AI models like LLaMA and Llama-2. It aims to make AI model handling more efficient by providing:\n",
        "\n",
        "- **Fast Loading**: Quickly loads models for immediate use.\n",
        "- **Flexibility**: Supports special tokens, metadata, and future extensibility without breaking compatibility.\n",
        "- **Single-File Convenience**: Packages entire models into one file, simplifying distribution and usage.\n",
        "\n",
        "This format is particularly useful for AI applications requiring efficient model loading and flexibility, such as those developed in PyTorch for inference with systems like llama.cpp. With its key-value structure for storing information, GGUF is more adaptable and easier to use compared to previous formats like GGML and GGJT."
      ],
      "metadata": {
        "id": "2hxzjkcBQnuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "cbnikyRvPMB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smHu8hADeYjQ",
        "outputId": "58b4eb45-a933-4e3b-e330-28ed3e055d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 19426, done.\u001b[K\n",
            "remote: Counting objects: 100% (5974/5974), done.\u001b[K\n",
            "remote: Compressing objects: 100% (277/277), done.\u001b[K\n",
            "remote: Total 19426 (delta 5845), reused 5711 (delta 5696), pack-reused 13452\u001b[K\n",
            "Receiving objects: 100% (19426/19426), 22.51 MiB | 21.34 MiB/s, done.\n",
            "Resolving deltas: 100% (13660/13660), done.\n"
          ]
        }
      ],
      "source": [
        "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
        "!git lfs install\n",
        "\n",
        "# Clone llama.cpp's repository. They provide code to convert models into gguf.\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone your model from Huggingface again.\n",
        "# Change the username to yours here.\n",
        "\n",
        "!git clone https://huggingface.co/dev02chandan/Llama-2-7b-chat-hf-fine-tuned"
      ],
      "metadata": {
        "id": "8ViqnrJ7DS7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cc7001-de01-4da5-c7b0-e146a420ac45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Llama-2-7b-chat-hf-fine-tuned'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 17 (delta 1), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (17/17), 482.26 KiB | 5.74 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 4.55 GiB | 11.75 MiB/s, done.\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00001-of-00003.safetensors\n",
            "\tmodel-00002-of-00003.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/llama.cpp/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CsKwgzp2ebw6",
        "outputId": "2ae83a1c-4588-40e7-e5d8-e4340c3e1242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy~=1.24.4 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Collecting transformers<5.0.0,>=4.35.2 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gguf>=0.1.0 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r /content/llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r /content/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r /content/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.31.0\n",
            "    Uninstalling transformers-4.31.0:\n",
            "      Successfully uninstalled transformers-4.31.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 tokenizers-0.15.2 torch-2.1.2 transformers-4.38.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0808531044ac4d04bde25cf393bab83e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/llama.cpp/convert.py /content/Llama-2-7b-chat-hf-fine-tuned \\\n",
        "   --vocab-type hfft \\\n",
        "  --outfile /content/finetuned.gguf \\\n",
        "  --outtype q8_0                        # this means you are quanitising the model to 8 bit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTY7gmYgSzIc",
        "outputId": "af491c1b-0a68-4487-9f60-12a66d3a2287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00001-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00001-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00002-of-00003.safetensors\n",
            "Loading model file /content/Llama-2-7b-chat-hf-fine-tuned/model-00003-of-00003.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned'))\n",
            "Found vocab files: {'tokenizer.model': PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('/content/Llama-2-7b-chat-hf-fine-tuned/tokenizer.json')}\n",
            "Loading vocab file '/content/Llama-2-7b-chat-hf-fine-tuned', type 'hfft'\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "fname_tokenizer: /content/Llama-2-7b-chat-hf-fine-tuned\n",
            "Vocab info: <HfVocab with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 0}, add special tokens unset>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "skipping tensor blk.0.attn_rot_embd\n",
            "skipping tensor blk.1.attn_rot_embd\n",
            "skipping tensor blk.10.attn_rot_embd\n",
            "skipping tensor blk.11.attn_rot_embd\n",
            "skipping tensor blk.2.attn_rot_embd\n",
            "skipping tensor blk.3.attn_rot_embd\n",
            "skipping tensor blk.4.attn_rot_embd\n",
            "skipping tensor blk.5.attn_rot_embd\n",
            "skipping tensor blk.6.attn_rot_embd\n",
            "skipping tensor blk.7.attn_rot_embd\n",
            "skipping tensor blk.8.attn_rot_embd\n",
            "skipping tensor blk.9.attn_rot_embd\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
            "skipping tensor blk.12.attn_rot_embd\n",
            "skipping tensor blk.13.attn_rot_embd\n",
            "skipping tensor blk.14.attn_rot_embd\n",
            "skipping tensor blk.15.attn_rot_embd\n",
            "skipping tensor blk.16.attn_rot_embd\n",
            "skipping tensor blk.17.attn_rot_embd\n",
            "skipping tensor blk.18.attn_rot_embd\n",
            "skipping tensor blk.19.attn_rot_embd\n",
            "skipping tensor blk.20.attn_rot_embd\n",
            "skipping tensor blk.21.attn_rot_embd\n",
            "skipping tensor blk.22.attn_rot_embd\n",
            "skipping tensor blk.23.attn_rot_embd\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
            "skipping tensor blk.24.attn_rot_embd\n",
            "skipping tensor blk.25.attn_rot_embd\n",
            "skipping tensor blk.26.attn_rot_embd\n",
            "skipping tensor blk.27.attn_rot_embd\n",
            "skipping tensor blk.28.attn_rot_embd\n",
            "skipping tensor blk.29.attn_rot_embd\n",
            "skipping tensor blk.30.attn_rot_embd\n",
            "skipping tensor blk.31.attn_rot_embd\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
            "Writing /content/finetuned.gguf, format 7\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 0\n",
            "gguf: Setting chat_template to {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
            "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+  35\n",
            "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  36\n",
            "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  36\n",
            "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  36\n",
            "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  36\n",
            "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  36\n",
            "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  37\n",
            "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  37\n",
            "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  39\n",
            "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  41\n",
            "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  41\n",
            "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  52\n",
            "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  56\n",
            "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  57\n",
            "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  58\n",
            "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  58\n",
            "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  58\n",
            "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  58\n",
            "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  58\n",
            "[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  58\n",
            "[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  71\n",
            "[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  73\n",
            "[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  75\n",
            "[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
            "[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  75\n",
            "[ 26/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  76\n",
            "[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  76\n",
            "[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  76\n",
            "[ 29/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  81\n",
            "[ 30/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  83\n",
            "[ 31/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  83\n",
            "[ 32/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  83\n",
            "[ 33/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  83\n",
            "[ 34/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  83\n",
            "[ 35/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  89\n",
            "[ 36/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  91\n",
            "[ 37/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  92\n",
            "[ 38/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  93\n",
            "[ 39/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  93\n",
            "[ 40/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  93\n",
            "[ 41/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  93\n",
            "[ 42/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  93\n",
            "[ 43/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  94\n",
            "[ 44/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  99\n",
            "[ 45/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 103\n",
            "[ 46/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 104\n",
            "[ 47/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+ 104\n",
            "[ 48/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 104\n",
            "[ 49/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 104\n",
            "[ 50/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 104\n",
            "[ 51/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 104\n",
            "[ 52/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+ 105\n",
            "[ 53/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 114\n",
            "[ 54/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 116\n",
            "[ 55/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 116\n",
            "[ 56/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+ 117\n",
            "[ 57/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 117\n",
            "[ 58/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 117\n",
            "[ 59/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 117\n",
            "[ 60/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 117\n",
            "[ 61/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+ 117\n",
            "[ 62/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 126\n",
            "[ 63/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 128\n",
            "[ 64/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 129\n",
            "[ 65/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+ 129\n",
            "[ 66/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 129\n",
            "[ 67/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 130\n",
            "[ 68/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 130\n",
            "[ 69/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 130\n",
            "[ 70/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+ 130\n",
            "[ 71/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 137\n",
            "[ 72/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 140\n",
            "[ 73/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 141\n",
            "[ 74/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+ 141\n",
            "[ 75/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 141\n",
            "[ 76/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 141\n",
            "[ 77/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 141\n",
            "[ 78/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 141\n",
            "[ 79/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 141\n",
            "[ 80/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 149\n",
            "[ 81/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 152\n",
            "[ 82/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 153\n",
            "[ 83/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 153\n",
            "[ 84/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 153\n",
            "[ 85/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 153\n",
            "[ 86/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 153\n",
            "[ 87/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 153\n",
            "[ 88/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 153\n",
            "[ 89/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 162\n",
            "[ 90/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 166\n",
            "[ 91/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 166\n",
            "[ 92/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 167\n",
            "[ 93/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 167\n",
            "[ 94/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 167\n",
            "[ 95/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 167\n",
            "[ 96/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 167\n",
            "[ 97/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 167\n",
            "[ 98/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+ 175\n",
            "[ 99/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+ 178\n",
            "[100/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+ 180\n",
            "[101/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 181\n",
            "[102/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+ 181\n",
            "[103/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+ 181\n",
            "[104/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+ 181\n",
            "[105/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+ 181\n",
            "[106/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+ 181\n",
            "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 188\n",
            "[108/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 192\n",
            "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+ 193\n",
            "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+ 193\n",
            "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 194\n",
            "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 195\n",
            "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 197\n",
            "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+ 198\n",
            "[115/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 198\n",
            "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 198\n",
            "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 198\n",
            "[118/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 198\n",
            "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+ 198\n",
            "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 205\n",
            "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 208\n",
            "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 209\n",
            "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+ 209\n",
            "[124/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 209\n",
            "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 209\n",
            "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 209\n",
            "[127/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 209\n",
            "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+ 210\n",
            "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 220\n",
            "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 222\n",
            "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 223\n",
            "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+ 223\n",
            "[133/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 223\n",
            "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 223\n",
            "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 223\n",
            "[136/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 224\n",
            "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 224\n",
            "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 231\n",
            "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 234\n",
            "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 235\n",
            "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 235\n",
            "[142/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 235\n",
            "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 235\n",
            "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 235\n",
            "[145/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 235\n",
            "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 235\n",
            "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 245\n",
            "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 246\n",
            "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 247\n",
            "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 248\n",
            "[151/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 248\n",
            "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 248\n",
            "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 248\n",
            "[154/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 249\n",
            "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 249\n",
            "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 253\n",
            "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 259\n",
            "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 259\n",
            "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 259\n",
            "[160/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[163/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 259\n",
            "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 259\n",
            "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 270\n",
            "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 272\n",
            "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 273\n",
            "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 273\n",
            "[169/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 273\n",
            "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 273\n",
            "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 273\n",
            "[172/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 273\n",
            "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 273\n",
            "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 280\n",
            "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 284\n",
            "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 285\n",
            "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 286\n",
            "[178/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 286\n",
            "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 286\n",
            "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 286\n",
            "[181/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 286\n",
            "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 286\n",
            "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 292\n",
            "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 296\n",
            "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 297\n",
            "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 297\n",
            "[187/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 297\n",
            "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 297\n",
            "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 297\n",
            "[190/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 298\n",
            "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 298\n",
            "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 304\n",
            "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 308\n",
            "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 310\n",
            "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 311\n",
            "[196/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 311\n",
            "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 311\n",
            "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 311\n",
            "[199/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 311\n",
            "[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 311\n",
            "[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 316\n",
            "[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 323\n",
            "[203/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 324\n",
            "[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 324\n",
            "[205/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 324\n",
            "[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 324\n",
            "[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 324\n",
            "[208/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 324\n",
            "[209/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 328\n",
            "[210/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 332\n",
            "[211/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 332\n",
            "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 333\n",
            "[213/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 333\n",
            "[214/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 333\n",
            "[215/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+ 351\n",
            "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 351\n",
            "[217/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 351\n",
            "[218/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 351\n",
            "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 351\n",
            "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 351\n",
            "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 351\n",
            "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 352\n",
            "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 354\n",
            "[224/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 354\n",
            "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 355\n",
            "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 356\n",
            "[227/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 356\n",
            "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 356\n",
            "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 362\n",
            "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 365\n",
            "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 366\n",
            "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 367\n",
            "[233/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 367\n",
            "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 367\n",
            "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 367\n",
            "[236/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 367\n",
            "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 367\n",
            "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 374\n",
            "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 377\n",
            "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 378\n",
            "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 378\n",
            "[242/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 378\n",
            "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 378\n",
            "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 379\n",
            "[245/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 379\n",
            "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 380\n",
            "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 384\n",
            "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 389\n",
            "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 390\n",
            "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 390\n",
            "[251/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 390\n",
            "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 390\n",
            "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 390\n",
            "[254/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 390\n",
            "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 391\n",
            "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 398\n",
            "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 401\n",
            "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 403\n",
            "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 403\n",
            "[260/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 403\n",
            "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 404\n",
            "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 404\n",
            "[263/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 404\n",
            "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 404\n",
            "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 410\n",
            "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 413\n",
            "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 414\n",
            "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 415\n",
            "[269/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 415\n",
            "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 416\n",
            "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 416\n",
            "[272/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 416\n",
            "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 416\n",
            "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 422\n",
            "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 425\n",
            "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 426\n",
            "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 426\n",
            "[278/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 426\n",
            "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 426\n",
            "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 426\n",
            "[281/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 426\n",
            "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 426\n",
            "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 433\n",
            "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 435\n",
            "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 435\n",
            "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 435\n",
            "[287/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 435\n",
            "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 436\n",
            "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 436\n",
            "[290/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 436\n",
            "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 436\n",
            "Wrote /content/finetuned.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "UaEdtsT8S8xp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "152764d940bf4c8c8c8e5ced0aaa4b2a",
            "6ed1b0dce51442638e4433f44091eaad",
            "cee7a37012a74a6aa944b332d2b368e9",
            "64a984f63cae42b1861f6487e35e8743",
            "452adeb933e54452a2cdd325fd6d511c",
            "f85e8bca046a46cbbf0eb409d1037fae",
            "9bd4446b4153494abf0d401be2e37684",
            "eb2fa6b59c35435facc87c4d9d6fc896",
            "49457a949e6944038d53e21d00b7adf5",
            "5f611c0caa34497da6ca1ffa795639f1",
            "5b9ae8dc83344560bc3b34834e0a0c81",
            "3af6d456d9884fb8b449d3f068aafbcd",
            "dad6b5fc5cbb460db8d81eb310bd6490",
            "b06f56d7d4fc40c38d28ff0913f6f7aa",
            "06e73fac57b84a60b38d52ba249fe15d",
            "2e3eab52885a4c1fa909b52cdd426036",
            "490aa7202c03440892651e7730f9a4bd",
            "0f121eb573be493fbeaab6179b4f9015",
            "a1662fbf845c48d3b2ddaef810f4848e",
            "0634228a6176415a920c6b53a994486a",
            "e101206572b24764847911b6601cf5c6",
            "124a3751eda2407cb506c8b2555278ee",
            "7e74217233f2417a9ca897691bfd7745",
            "84e0105085a44446a4357da14761736c",
            "70c55cd669134cd68b211212c3d94397",
            "05a5b2fe5efd4349937887cca0e18b55",
            "cc3360d483764597a1d7100ef61de715",
            "905f147d6bfc4a17999cdc13df18b58b",
            "860c81186dcb45609830cf269f33c83d",
            "13b90f3e762846e9bf66c6f6dc9954de",
            "3a72ab1f08a34c02ad3db2da53e27127",
            "162b6df5011145f88c98efdeaf62e0c2"
          ]
        },
        "outputId": "6bb390f6-bace-4b95-fcca-e5f23cabf454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "152764d940bf4c8c8c8e5ced0aaa4b2a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_USERNAME = 'dev02chandan'"
      ],
      "metadata": {
        "id": "6qvlccVqSzmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will push the new GGUF model to HF repository\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = f\"{HUGGING_FACE_USERNAME}/Llama-2-7b-chat-hf-finedtuned-to-GGUF\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"finetuned.gguf\",\n",
        "    path_in_repo=\"finetuned.gguf\",\n",
        "    repo_id=model_id,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "a1961dd237a542cc822f3acbff1ea501",
            "5bf1d62a1f6b42e08487b98ddd8aab3b",
            "4cbb7bff2dd245798d9e3d5085c3a9cf",
            "8b6ff220a8a8457daee9a1d9ddd0ce70",
            "41a03c2d8e9e4e18a8227a73a6b9f4a5",
            "c64c8d01e7374c0abc45538ba0317f60",
            "a3d01d00c7aa4cfe8cf6bb7bb2cdb5bf",
            "3234c04b00e840ffae72c07f1953fa2d",
            "90236bcfce1e49c29461578141a8bcc7",
            "751d4ea8761640ae909300a39c7fa297",
            "e4f563d7b2ca4e20937c7e801d08ebb6"
          ]
        },
        "id": "JYY0mz6qm1-j",
        "outputId": "b14dab1e-336c-438d-96c5-c92bad18429b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "finetuned.gguf:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1961dd237a542cc822f3acbff1ea501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/dev02chandan/Llama-2-7b-chat-hf-finedtuned-to-GGUF/commit/82c62bf77f04006120f6035fa01b3aa5da7ff26d', commit_message='Upload finetuned.gguf with huggingface_hub', commit_description='', oid='82c62bf77f04006120f6035fa01b3aa5da7ff26d', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the model has been correctly pushed to Hf, by clicking on the link above. You should has check the size of the model is about 7.16G if you are using Llama and 8 bit quantisation."
      ],
      "metadata": {
        "id": "BXZQUxI_pmqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can remove the cloned repo now, since it was only used for getting the model into gguf format.\n",
        "# This will free up some disk space\n",
        "!rm -rf llama.cpp"
      ],
      "metadata": {
        "id": "jq3Ep9qiqLjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference from GGUF**"
      ],
      "metadata": {
        "id": "arSO1EZuXJhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you are using Free colab - this will crash (2nd time), if you have ran all previous cells.**\n",
        "\n",
        " ***`IMP`***\n",
        "\n",
        "**Going Ahead - You can delete and disconnect runtime. Start a new runtime with GPU if your model is saved to Hugging face. The code ahead will help you reload the model and move on.**"
      ],
      "metadata": {
        "id": "5HdRtYV2o21M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can infer without using GPU, but the inference speed will be extremely slow"
      ],
      "metadata": {
        "id": "zo3QyiI2D1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base ctransformers with no GPU acceleration\n",
        "!pip install ctransformers>=0.2.24\n",
        "# Or with CUDA GPU acceleration\n",
        "!pip install ctransformers[cuda]>=0.2.24\n",
        "# Or with ROCm GPU acceleration\n",
        "!CT_HIPBLAS=1 pip install ctransformers>=0.2.24 --no-binary ctransformers\n",
        "# Or with Metal GPU acceleration for macOS systems\n",
        "!CT_METAL=1 pip install ctransformers>=0.2.24 --no-binary ctransformers"
      ],
      "metadata": {
        "id": "pOE3FGjsXVY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how to call the model from Hugging Face, and learn to infer from it."
      ],
      "metadata": {
        "id": "IWtjkbcnPvtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_USERNAME = 'dev02chandan'"
      ],
      "metadata": {
        "id": "dUHawE5wp_ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    f\"{HUGGING_FACE_USERNAME}/Llama-2-7b-chat-hf-finedtuned-to-GGUF\",\n",
        "    model_file=\"finetuned.gguf\",\n",
        "    model_type=\"llama\",\n",
        "    gpu_layers = 100,             # Set this to 0 if you have only a CPU (Inference is pretty slow)\n",
        "    max_new_tokens = 2000,\n",
        "    temperature = 0.2,\n",
        "    top_k = 40,\n",
        "    top_p = 0.6,\n",
        "    context_length = 6000\n",
        ")"
      ],
      "metadata": {
        "id": "oUOzkM9LXYwO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "203251465dd04410a1aab08ad64f23b7",
            "acdc21c1d54d40c892618557e73709af",
            "04e8d927827345a7bc35f868634bcd7c",
            "37375379aced4d8a87fbf57e753d26ea",
            "f337b2503e36468cad2fa08d447f8d3f",
            "2dcaedde141c4ed1ae87ce4ddf0c44ed",
            "ad2d81d949f84c169e53f5fca3b76d62",
            "f0cc8b156fac41de88ad674825824bc8",
            "2322ebd8239d4ce5b368a26bf4f47a84",
            "031e3da1e28e4759978494f717167fa0",
            "0d2ba5f1e8184bd5a236a50f8bcc7884",
            "f7baa279cc1442e0b88738aa7ef8e026",
            "a3bc1f07da714f81b1c2eb5e9949bd1b",
            "8263dba1471f4f698c5f8a590130476e",
            "aa92f30fb22f47c4ab2f1120de8a773a",
            "5276139994c54e3ba7dd951da418ebed",
            "713bb9ae185c419ebc82945dac9e945e",
            "43400c145f344eb28d359b5fb8e204f2",
            "71f55495b24b44a1a58f7c74f0061816",
            "000513c0f10246ec8f2b878878d68f0a",
            "c35318af0f4e46028d270bde25abfa0e",
            "fd753e3fa2bd40c4a0e90c6ca90a8a50",
            "c941be70bf3444d5beeadf2f309a7942",
            "be3d5bb7542b4f1b9de95f7482a7a7ae",
            "3842cc0a27e74c549eeaeb55f7d1e150",
            "c585da92c73e482e923230c925d30266",
            "0c1e9bb90bb9469eaeaa65a0e3ff0a1d",
            "97140fa16af64a019385678e846bed10",
            "a9f1a542abf846389ac46724e30968ac",
            "2b5203b1b4f94a408620089be5bbbc71",
            "dabb85d7b4854c64927c497d10499539",
            "ae870a30c4b845588984156e34ff8da8",
            "7c0210ead85b424ab6e552cf624fb7b2"
          ]
        },
        "outputId": "aefbd9da-39e4-4692-c1dc-943f01ce85dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "203251465dd04410a1aab08ad64f23b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7baa279cc1442e0b88738aa7ef8e026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "finetuned.gguf:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c941be70bf3444d5beeadf2f309a7942"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = '''<<SYS>>\n",
        "As a chatbot for the Company 'Maitri Lab Grown Diamonds', your primary focus is to provide concise, accurate and helpful information about the company Maitri Lab Grown diamonds and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
        "<</SYS>>\n",
        "\n",
        " '''\n",
        "\n",
        "# Try Changing this\n",
        "user_prompt = \"Tell me about your company\"\n",
        "\n",
        "# Combine system prompt with user prompt\n",
        "full_prompt = f\"{system_prompt}\\n[INST]{user_prompt}[/INST]\"\n",
        "\n",
        "print(full_prompt)"
      ],
      "metadata": {
        "id": "s3LEKZ9MXmNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ebe116-1079-4eb8-d962-1aa8c2e14999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<SYS>>\n",
            "As a chatbot for the Company 'Maitri Lab Grown Diamonds', your primary focus is to provide concise, accurate and helpful information about the company Maitri Lab Grown diamonds and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
            "<</SYS>>\n",
            "\n",
            " \n",
            "[INST]Tell me about your company[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the response\n",
        "response = llm(full_prompt)\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "id": "jd9J24eBXulm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03f99cf-f7a7-4c09-d9a6-d46f82405640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Of course! I'm happy to tell you more about Maitri Lab Grown Diamonds.\n",
            "\n",
            "Maitri Lab Grown Diamonds is a leading manufacturer and supplier of high-quality lab-grown diamonds. Our mission is to provide ethical, sustainable, and environmentally friendly diamond options to our customers while maintaining the highest standards of quality and craftsmanship.\n",
            "\n",
            "At Maitri, we believe that everyone deserves to own a beautiful and unique diamond without compromising on ethical or environmental considerations. That's why we use advanced technology to grow our diamonds in a controlled environment, eliminating the need for mining and minimizing the impact on the environment.\n",
            "\n",
            "Our team of experts works closely with suppliers and partners to ensure that every aspect of our production process meets the highest standards of quality and ethical sourcing. We offer a wide range of lab-grown diamond options, including round brilliant cuts, princess cuts, cushion cuts, and more.\n",
            "\n",
            "In addition to our commitment to sustainability and ethical practices, we also prioritize customer satisfaction and provide exceptional service to ensure that every customer has a positive experience with Maitri Lab Grown Diamonds.\n",
            "\n",
            "So, whether you're looking for an engagement ring, a special occasion piece, or simply a beautiful diamond to add to your collection, Maitri Lab Grown Diamonds is the perfect choice. Thank you for considering us!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference from GGUF using Langchain (Conversational Memory)**"
      ],
      "metadata": {
        "id": "J8yk0KWDYFaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_USERNAME = 'dev02chandan'"
      ],
      "metadata": {
        "id": "KRNomGn3rB6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import CTransformers\n",
        "\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "config = {\n",
        "    'gpu_layers' : 50,\n",
        "    'max_new_tokens' : 2000,\n",
        "    'context_length':6000,\n",
        "    'temperature' : 0.2,\n",
        "    'top_k' :40,\n",
        "    'top_p' : 0.6\n",
        "    }\n",
        "\n",
        "\n",
        "llm = CTransformers(\n",
        "    model = f\"{HUGGING_FACE_USERNAME}/Llama-2-7b-chat-hf-finedtuned-to-GGUF\",\n",
        "    model_file=\"finetuned.gguf\",\n",
        "    model_type=\"llama\",\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    config = config\n",
        ")"
      ],
      "metadata": {
        "id": "qwuOb5i-YJJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "fec8148bdea441ba82d508658a6eced9",
            "3a058d1b179b48fa9f59f6f9c11b3729",
            "36326a5f8e814aae986fab3c8311b7b8",
            "e6f622dc6093446aa256bdf1a826ca79",
            "679bd161d4724767bb63de51bd6377ba",
            "3bc31676d3e94f0c8abdc317a2cb020a",
            "7119549babff419e8b6d26ac587e14d8",
            "7678c2d0343b48839d46715879c6f30b",
            "1f86aee5d39a407cbc3f4359dc2fd451",
            "4b72f724486c4137943c418b42d35dbe",
            "aefd91c7e1874fb79f83adcc6bc02f59",
            "b30ed5ce4a354050b66b017739807cbf",
            "e660ab76d12941a8b387acf44c404f0b",
            "19730800ba1943f497fde976d5c8123d",
            "00af510bedd8432fb250c2227ddbbfa5",
            "6936660574f344c481dbfc8e174cebf6",
            "61323d950290407c8a57fefc94488dd2",
            "5b89e8c2c7314bd68c5c016bc99554ef",
            "cfe5414e9a8548a19ef7b6444cb88c73",
            "fa8ee82920c245d08d490daba55eb480",
            "23332927d76a48bda6c04963e8499397",
            "cdd8efca855c43aa9592e0740d0a1b32"
          ]
        },
        "outputId": "733a523a-3210-465a-e323-c0c2e879f6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fec8148bdea441ba82d508658a6eced9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b30ed5ce4a354050b66b017739807cbf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template with placeholders for dynamic content insertion\n",
        "template = '''\n",
        "[INST]\n",
        "<<SYS>>\n",
        "As a chatbot for the Company 'Maitri Lab Grown Diamonds', your primary focus is to provide concise, accurate and helpful information about the company Maitri Lab Grown diamonds and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
        "<</SYS>>\n",
        "\n",
        "{question}[/INST]\n",
        "'''"
      ],
      "metadata": {
        "id": "fJGfMdwhYWCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PromptTemplate instance from the template\n",
        "prompt = PromptTemplate(template=template,  input_variables = ['question'])"
      ],
      "metadata": {
        "id": "wim4Jm0dYdwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "s8FhUIJqYmV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try changing this\n",
        "response = llm_chain.invoke(\"Hi\")"
      ],
      "metadata": {
        "id": "5Hpaqw02YoQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9677dd82-c96c-4e15-9445-b6da9e801096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm happy to help you with any questions you may have about Mai Tri Lab Grown Diamonds or the diamond industry. Please feel free to ask me anything, and I will do my best to provide you with concise, accurate, and helpful information. Whether you're interested in learning more about our company, the different types of lab-grown diamonds we offer, or how to properly care for your diamond jewelry, I'm here to help. If you have any specific questions or topics in mind, please don't hesitate to ask!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke(\"Difference between CVD and lab grown diamonds?\")"
      ],
      "metadata": {
        "id": "ePiUGb73ZELb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad410a84-3dc6-4bbf-db1c-1d69ee52d88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thank you for reaching out! As a chatbot for Maitri Lab Grown Diamonds, I'm happy to help you understand the difference between CVD (Cultured Vs Lab Grown) and Lab Grown Diamonds.\n",
            "\n",
            "CVD refers to diamonds that are grown using a process called High Pressure High Temperature (HPHT), which replicates the natural geological processes that form diamonds in the earth's crust. These diamonds are also known as Cultured Diamonds or Artificial Diamonds.\n",
            "\n",
            "On the other hand, Lab Grown Diamonds are created using a process called Chemical Vapor Deposition (CVD), which involves the deposition of carbon atoms onto a substrate in a vacuum chamber. This process replicates the natural process of diamond formation in the earth's mantle, but it is done in a laboratory setting.\n",
            "\n",
            "The main difference between CVD and Lab Grown Diamonds is the way they are grown. CVD uses HPHT to create diamonds, while Lab Grown Diamonds use CVD. Both methods produce diamonds that are chemically and optically identical to natural diamonds, but they have different growth processes.\n",
            "\n",
            "Maitri Lab Grown Diamonds are created using the CVD process, which allows for greater control over the growth conditions and results in a more consistent product. Our diamonds are also certified by independent labs, such as the Gemological Institute of America (GIA), to ensure their quality and authenticity.\n",
            "\n",
            "I hope this information helps you understand the difference between CVD and Lab Grown Diamonds! If you have any other questions or would like to learn more about Maitri Lab Grown Diamonds, please feel free to ask."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Add memory\n",
        "\n",
        "# k indicates the number of questions and answers the llm remembers\n",
        "# this is simple done by passing the questions and answers that it gave previously again in the system prompt\n",
        "# langchain makes this operation simple"
      ],
      "metadata": {
        "id": "b8slbtGoZGBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define the template with placeholders for dynamic content insertion\n",
        "template = '''\n",
        "[INST]\n",
        "<>\n",
        "As a chatbot for the Company 'Maitri Lab Grown Diamonds', your primary focus is to provide concise, accurate and helpful information about the company Maitri Lab Grown diamonds and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
        "\n",
        "Previous conversation : {chat_history}\n",
        "\n",
        "<>\n",
        "\n",
        "{question}[/INST]'''"
      ],
      "metadata": {
        "id": "HpaQecXHvKd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PromptTemplate instance from the template\n",
        "prompt = PromptTemplate(template=template,  input_variables = [\"question\"])\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=2\n",
        ")"
      ],
      "metadata": {
        "id": "YVbJmZDAvQjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose = False         # This limits the LLM from describing the answer too much.\n",
        ")"
      ],
      "metadata": {
        "id": "ynL1ImbHZLAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(\"Tell me 4 points without details about your company.\")"
      ],
      "metadata": {
        "id": "yp1OtGkzZPPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b1b7b0-41e3-4bf8-bfb1-150ac8c97080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Of course! As a representative of Maitri Lab Grown Diamonds, I'd be happy to provide you with some key points about our company:\n",
            "\n",
            "1. Sustainability: We prioritize sustainability in every aspect of our business, from the sourcing of our diamonds to the packaging and shipping of our products.\n",
            "2. Ethical practices: We adhere to ethical practices throughout our supply chain, ensuring that our diamonds are conflict-free and sourced responsibly.\n",
            "3. High-quality diamonds: Our lab-grown diamonds are of exceptional quality, with unparalleled brilliance and fire.\n",
            "4. Innovative technology: We use cutting-edge technology to create our diamonds, ensuring that they are not only beautiful but also of the highest quality."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Tell me 4 points without details about your company.',\n",
              " 'chat_history': '',\n",
              " 'text': \"  Of course! As a representative of Maitri Lab Grown Diamonds, I'd be happy to provide you with some key points about our company:\\n\\n1. Sustainability: We prioritize sustainability in every aspect of our business, from the sourcing of our diamonds to the packaging and shipping of our products.\\n2. Ethical practices: We adhere to ethical practices throughout our supply chain, ensuring that our diamonds are conflict-free and sourced responsibly.\\n3. High-quality diamonds: Our lab-grown diamonds are of exceptional quality, with unparalleled brilliance and fire.\\n4. Innovative technology: We use cutting-edge technology to create our diamonds, ensuring that they are not only beautiful but also of the highest quality.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does it remember what it just said?"
      ],
      "metadata": {
        "id": "fQHc1WqwwGr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(\"Explain the 2nd point from your previous answer.\")"
      ],
      "metadata": {
        "id": "zITwebPhZeZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9367a7a2-1f18-4db5-ede8-dd6cb56e2c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Of course! The second point I mentioned is that Maitri Lab Grown Diamonds adheres to ethical practices throughout our supply chain. This means that we prioritize responsible and conflict-free sourcing of our diamonds, ensuring that they are free from any unethical or illegal activities.\n",
            "\n",
            "At Maitri, we understand the importance of transparency and accountability in the diamond industry, and we take great care to ensure that our diamonds are sourced in a responsible and sustainable manner. This includes working with suppliers who adhere to strict ethical standards, such as the Kimberley Process Certification Scheme, which aims to prevent the trade of conflict diamonds.\n",
            "\n",
            "By prioritizing ethical practices throughout our supply chain, we can ensure that our customers can trust in the quality and integrity of our diamonds, and feel confident that they are making a responsible purchase."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Explain the 2nd point from your previous answer.',\n",
              " 'chat_history': \"Human: Tell me 4 points without details about your company.\\nAI:   Of course! As a representative of Maitri Lab Grown Diamonds, I'd be happy to provide you with some key points about our company:\\n\\n1. Sustainability: We prioritize sustainability in every aspect of our business, from the sourcing of our diamonds to the packaging and shipping of our products.\\n2. Ethical practices: We adhere to ethical practices throughout our supply chain, ensuring that our diamonds are conflict-free and sourced responsibly.\\n3. High-quality diamonds: Our lab-grown diamonds are of exceptional quality, with unparalleled brilliance and fire.\\n4. Innovative technology: We use cutting-edge technology to create our diamonds, ensuring that they are not only beautiful but also of the highest quality.\",\n",
              " 'text': '  Of course! The second point I mentioned is that Maitri Lab Grown Diamonds adheres to ethical practices throughout our supply chain. This means that we prioritize responsible and conflict-free sourcing of our diamonds, ensuring that they are free from any unethical or illegal activities.\\n\\nAt Maitri, we understand the importance of transparency and accountability in the diamond industry, and we take great care to ensure that our diamonds are sourced in a responsible and sustainable manner. This includes working with suppliers who adhere to strict ethical standards, such as the Kimberley Process Certification Scheme, which aims to prevent the trade of conflict diamonds.\\n\\nBy prioritizing ethical practices throughout our supply chain, we can ensure that our customers can trust in the quality and integrity of our diamonds, and feel confident that they are making a responsible purchase.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if it still remembers its first response"
      ],
      "metadata": {
        "id": "-VZuexezwC1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(\"Now explain the first point please.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us5jl-Rwvz2q",
        "outputId": "d49503f0-05c8-4877-fe39-f3f87405c783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Of course! The first point I mentioned is that Maitri Lab Grown Diamonds prioritizes sustainability in every aspect of our business. This means that we take a holistic approach to sustainability, considering not only the environmental impact of our operations but also the social and economic implications of our actions.\n",
            "\n",
            "At Maitri, we believe that sustainability is not just a buzzword or a marketing strategy, but rather a core value that guides everything we do. We strive to minimize our environmental footprint through efficient use of resources, reduction of waste, and sourcing of materials from responsible suppliers.\n",
            "\n",
            "But sustainability goes beyond just environmental considerations. We also prioritize social responsibility, ensuring that our business practices promote fair labor standards, ethical sourcing, and community development. By prioritizing sustainability in every aspect of our operations, we can create a more responsible and ethical diamond industry that benefits both people and the planet."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Now explain the first point please.',\n",
              " 'chat_history': \"Human: Tell me 4 points without details about your company.\\nAI:   Of course! As a representative of Maitri Lab Grown Diamonds, I'd be happy to provide you with some key points about our company:\\n\\n1. Sustainability: We prioritize sustainability in every aspect of our business, from the sourcing of our diamonds to the packaging and shipping of our products.\\n2. Ethical practices: We adhere to ethical practices throughout our supply chain, ensuring that our diamonds are conflict-free and sourced responsibly.\\n3. High-quality diamonds: Our lab-grown diamonds are of exceptional quality, with unparalleled brilliance and fire.\\n4. Innovative technology: We use cutting-edge technology to create our diamonds, ensuring that they are not only beautiful but also of the highest quality.\\nHuman: Explain the 2nd point from your previous answer.\\nAI:   Of course! The second point I mentioned is that Maitri Lab Grown Diamonds adheres to ethical practices throughout our supply chain. This means that we prioritize responsible and conflict-free sourcing of our diamonds, ensuring that they are free from any unethical or illegal activities.\\n\\nAt Maitri, we understand the importance of transparency and accountability in the diamond industry, and we take great care to ensure that our diamonds are sourced in a responsible and sustainable manner. This includes working with suppliers who adhere to strict ethical standards, such as the Kimberley Process Certification Scheme, which aims to prevent the trade of conflict diamonds.\\n\\nBy prioritizing ethical practices throughout our supply chain, we can ensure that our customers can trust in the quality and integrity of our diamonds, and feel confident that they are making a responsible purchase.\",\n",
              " 'text': '  Of course! The first point I mentioned is that Maitri Lab Grown Diamonds prioritizes sustainability in every aspect of our business. This means that we take a holistic approach to sustainability, considering not only the environmental impact of our operations but also the social and economic implications of our actions.\\n\\nAt Maitri, we believe that sustainability is not just a buzzword or a marketing strategy, but rather a core value that guides everything we do. We strive to minimize our environmental footprint through efficient use of resources, reduction of waste, and sourcing of materials from responsible suppliers.\\n\\nBut sustainability goes beyond just environmental considerations. We also prioritize social responsibility, ensuring that our business practices promote fair labor standards, ethical sourcing, and community development. By prioritizing sustainability in every aspect of our operations, we can create a more responsible and ethical diamond industry that benefits both people and the planet.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = conversation(\"What is the speciality of Maitri Lab Grown Diamonds?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqhqPN3usIN9",
        "outputId": "36a89ccf-80cd-410e-f973-e113eb7f2336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hello! I'm happy to help you with your question. As a chatbot for Mairi Lab Grown Diamonds, I can tell you that our company specializes in providing high-quality, ethically sourced diamonds that are grown using advanced technology.\n",
            "\n",
            "Unlike traditional mined diamonds, which have environmental and social impacts, our lab-grown diamonds are created using a proprietary process that minimizes waste and energy consumption. This makes them a more sustainable choice for consumers who want to make a responsible purchase.\n",
            "\n",
            "In addition to their ethical sourcing, Mairi Lab Grown Diamonds are also known for their exceptional quality and brilliance. Our team of experts carefully selects and grows each diamond to ensure that it meets the highest standards of clarity, color, and cut.\n",
            "\n",
            "Overall, Mairi Lab Grown Diamonds offer a unique combination of sustainability, ethics, and quality that sets us apart from other diamond companies. Whether you're looking for a diamond for personal use or as an investment, we believe that our lab-grown diamonds are the perfect choice."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C89oL4ScswcM",
        "outputId": "3908dd3a-5454-400c-8e15-cb4b83e66eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the speciality of Maitri Lab Grown Diamonds?',\n",
              " 'chat_history': 'Human: Explain the 2nd point from your previous answer.\\nAI:   Of course! The second point I mentioned is that Maitri Lab Grown Diamonds adheres to ethical practices throughout our supply chain. This means that we prioritize responsible and conflict-free sourcing of our diamonds, ensuring that they are free from any unethical or illegal activities.\\n\\nAt Maitri, we understand the importance of transparency and accountability in the diamond industry, and we take great care to ensure that our diamonds are sourced in a responsible and sustainable manner. This includes working with suppliers who adhere to strict ethical standards, such as the Kimberley Process Certification Scheme, which aims to prevent the trade of conflict diamonds.\\n\\nBy prioritizing ethical practices throughout our supply chain, we can ensure that our customers can trust in the quality and integrity of our diamonds, and feel confident that they are making a responsible purchase.\\nHuman: Now explain the first point please.\\nAI:   Of course! The first point I mentioned is that Maitri Lab Grown Diamonds prioritizes sustainability in every aspect of our business. This means that we take a holistic approach to sustainability, considering not only the environmental impact of our operations but also the social and economic implications of our actions.\\n\\nAt Maitri, we believe that sustainability is not just a buzzword or a marketing strategy, but rather a core value that guides everything we do. We strive to minimize our environmental footprint through efficient use of resources, reduction of waste, and sourcing of materials from responsible suppliers.\\n\\nBut sustainability goes beyond just environmental considerations. We also prioritize social responsibility, ensuring that our business practices promote fair labor standards, ethical sourcing, and community development. By prioritizing sustainability in every aspect of our operations, we can create a more responsible and ethical diamond industry that benefits both people and the planet.',\n",
              " 'text': \"  Hello! I'm happy to help you with your question. As a chatbot for Mairi Lab Grown Diamonds, I can tell you that our company specializes in providing high-quality, ethically sourced diamonds that are grown using advanced technology.\\n\\nUnlike traditional mined diamonds, which have environmental and social impacts, our lab-grown diamonds are created using a proprietary process that minimizes waste and energy consumption. This makes them a more sustainable choice for consumers who want to make a responsible purchase.\\n\\nIn addition to their ethical sourcing, Mairi Lab Grown Diamonds are also known for their exceptional quality and brilliance. Our team of experts carefully selects and grows each diamond to ensure that it meets the highest standards of clarity, color, and cut.\\n\\nOverall, Mairi Lab Grown Diamonds offer a unique combination of sustainability, ethics, and quality that sets us apart from other diamond companies. Whether you're looking for a diamond for personal use or as an investment, we believe that our lab-grown diamonds are the perfect choice.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output is in the form of a dictionary and you can see how the previous prompts are added to the next one along with the answers.\n",
        "\n",
        "You can increase the value of K as per your application."
      ],
      "metadata": {
        "id": "kwEFCJjPsQAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Finally, Your LLM with a Frontend (Chainlit using Ngrok)**"
      ],
      "metadata": {
        "id": "sdMuijRNZaCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can run this on Local as well, but CPU Inference will be very slow.\n",
        "# This code writes all our important code of calling the GGUF model, and then connects it\n",
        "# to a chainlit frontend\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "import chainlit as cl\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "\n",
        "\n",
        "# Configuration for the language model\n",
        "config = {\n",
        "    'gpu_layers': 100,\n",
        "    'max_new_tokens': 500,\n",
        "    'context_length': 4000,\n",
        "    'temperature': 0.2,\n",
        "    'top_k': 40,\n",
        "    'top_p': 0.6\n",
        "}\n",
        "\n",
        "llm = CTransformers(\n",
        "    model = \"vishanoberoi/Llama-2-7b-chat-hf-finedtuned-to-GGUF\",\n",
        "    model_file=\"finetuned.gguf\",\n",
        "    model_type=\"llama\",\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    config = config\n",
        "    )\n",
        "\n",
        "\n",
        "# Define the prompt template\n",
        "template = '''\n",
        "[INST]\n",
        "<<SYS>>\n",
        "As a chatbot for the Company \"Maitri Lab Grown Diamonds,\" your primary focus is to provide concise, accurate, and helpful information about the company and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise, or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
        "Be humble.\n",
        "Respond to the following question as a chatbot for the company named \"MAITRI LAB GROWN DIAMONDS.\"\n",
        "<</SYS>>\n",
        "\n",
        "{question}[/INST]'''\n",
        "\n",
        "# Define the chat start function\n",
        "@cl.on_chat_start\n",
        "def on_chat_start():\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
        "\n",
        "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
        "\n",
        "\n",
        "# Define the message function\n",
        "@cl.on_message\n",
        "async def on_message(message: cl.Message):\n",
        "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
        "\n",
        "\n",
        "    cb = cl.AsyncLangchainCallbackHandler(\n",
        "        stream_final_answer=True\n",
        "    )\n",
        "    res = await llm_chain.acall(message.content, callbacks = [cb])\n",
        "\n",
        "\n",
        "    await cl.Message(content=res['text']).send()"
      ],
      "metadata": {
        "id": "wdDmR37AZpFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a815052e-bec1-40e8-feb9-5d54d5578f2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the NGROK key in colab Secrets"
      ],
      "metadata": {
        "id": "MIasusi9wqxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 'YOUR_NGROK_KEY_HERE'"
      ],
      "metadata": {
        "id": "f74xqTEzaX9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a573afc-4486-4516-822a-c20bab8820d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "V8RcBgkSaeYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecac8153-4db6-425b-f718-95ccb4f211a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://015f-34-16-160-201.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT NOTE:\n",
        "# click on the above URL (Public URL given by Ngrok) AFTER running the cell below (chainlit run app.py)\n",
        "# You can even share this Public URL (Ngrok one) to any device to test your model with the frontend"
      ],
      "metadata": {
        "id": "sLoeI8K-sgnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run app.py"
      ],
      "metadata": {
        "id": "i5VQZc5Tagta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c514fff7-8275-4293-c837-e05b8f45d4cf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/__init__.py:548: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import CTransformers`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]\n",
            "config.json: 100% 183/183 [00:00<00:00, 780kB/s]\n",
            "Fetching 1 files: 100% 1/1 [00:00<00:00,  3.48it/s]\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]\n",
            "finetuned.gguf:   0% 0.00/7.16G [00:00<?, ?B/s]\u001b[A\n",
            "finetuned.gguf:   0% 10.5M/7.16G [00:00<03:56, 30.3MB/s]\u001b[A\n",
            "finetuned.gguf:   0% 21.0M/7.16G [00:00<02:25, 49.0MB/s]\u001b[A\n",
            "finetuned.gguf:   0% 31.5M/7.16G [00:00<01:51, 63.8MB/s]\u001b[A\n",
            "finetuned.gguf:   1% 52.4M/7.16G [00:00<01:21, 87.5MB/s]\u001b[A\n",
            "finetuned.gguf:   1% 73.4M/7.16G [00:00<01:11, 99.6MB/s]\u001b[A\n",
            "finetuned.gguf:   1% 94.4M/7.16G [00:01<01:03, 111MB/s] \u001b[A\n",
            "finetuned.gguf:   2% 115M/7.16G [00:01<01:01, 115MB/s] \u001b[A\n",
            "finetuned.gguf:   2% 136M/7.16G [00:01<01:01, 115MB/s]\u001b[A\n",
            "finetuned.gguf:   2% 157M/7.16G [00:01<00:55, 126MB/s]\u001b[A\n",
            "finetuned.gguf:   2% 178M/7.16G [00:01<00:52, 132MB/s]\u001b[A\n",
            "finetuned.gguf:   3% 199M/7.16G [00:01<00:51, 135MB/s]\u001b[A\n",
            "finetuned.gguf:   3% 220M/7.16G [00:02<00:51, 135MB/s]\u001b[A\n",
            "finetuned.gguf:   3% 241M/7.16G [00:02<00:52, 131MB/s]\u001b[A\n",
            "finetuned.gguf:   4% 262M/7.16G [00:02<00:52, 132MB/s]\u001b[A\n",
            "finetuned.gguf:   4% 283M/7.16G [00:02<00:50, 136MB/s]\u001b[A\n",
            "finetuned.gguf:   4% 304M/7.16G [00:02<00:53, 129MB/s]\u001b[A\n",
            "finetuned.gguf:   5% 325M/7.16G [00:03<01:15, 90.2MB/s]\u001b[A\n",
            "finetuned.gguf:   5% 346M/7.16G [00:03<01:08, 99.9MB/s]\u001b[A\n",
            "finetuned.gguf:   5% 367M/7.16G [00:03<01:04, 106MB/s] \u001b[A\n",
            "finetuned.gguf:   5% 388M/7.16G [00:03<01:00, 112MB/s]\u001b[A\n",
            "finetuned.gguf:   6% 409M/7.16G [00:03<00:55, 122MB/s]\u001b[A\n",
            "finetuned.gguf:   6% 430M/7.16G [00:05<04:04, 27.5MB/s]\u001b[A\n",
            "finetuned.gguf:   6% 451M/7.16G [00:05<03:02, 36.7MB/s]\u001b[A\n",
            "finetuned.gguf:   7% 472M/7.16G [00:06<02:25, 46.1MB/s]\u001b[A\n",
            "finetuned.gguf:   7% 493M/7.16G [00:06<01:56, 57.4MB/s]\u001b[A\n",
            "finetuned.gguf:   7% 514M/7.16G [00:06<01:37, 68.4MB/s]\u001b[A\n",
            "finetuned.gguf:   7% 535M/7.16G [00:06<01:23, 79.2MB/s]\u001b[A\n",
            "finetuned.gguf:   8% 556M/7.16G [00:06<01:17, 85.2MB/s]\u001b[A\n",
            "finetuned.gguf:   8% 577M/7.16G [00:07<01:12, 91.2MB/s]\u001b[A\n",
            "finetuned.gguf:   8% 598M/7.16G [00:07<01:09, 95.0MB/s]\u001b[A\n",
            "finetuned.gguf:   9% 619M/7.16G [00:07<01:01, 106MB/s] \u001b[A\n",
            "finetuned.gguf:   9% 640M/7.16G [00:07<00:57, 114MB/s]\u001b[A\n",
            "finetuned.gguf:   9% 661M/7.16G [00:07<00:54, 119MB/s]\u001b[A\n",
            "finetuned.gguf:  10% 682M/7.16G [00:07<00:52, 123MB/s]\u001b[A\n",
            "finetuned.gguf:  10% 703M/7.16G [00:07<00:51, 125MB/s]\u001b[A\n",
            "finetuned.gguf:  10% 724M/7.16G [00:08<00:49, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  10% 744M/7.16G [00:08<00:57, 111MB/s]\u001b[A\n",
            "finetuned.gguf:  11% 765M/7.16G [00:08<00:58, 109MB/s]\u001b[A\n",
            "finetuned.gguf:  11% 786M/7.16G [00:08<00:58, 109MB/s]\u001b[A\n",
            "finetuned.gguf:  11% 807M/7.16G [00:08<00:58, 109MB/s]\u001b[A\n",
            "finetuned.gguf:  12% 828M/7.16G [00:09<00:54, 115MB/s]\u001b[A\n",
            "finetuned.gguf:  12% 849M/7.16G [00:09<00:52, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  12% 870M/7.16G [00:09<00:50, 125MB/s]\u001b[A\n",
            "finetuned.gguf:  12% 891M/7.16G [00:10<02:40, 39.0MB/s]\u001b[A\n",
            "finetuned.gguf:  13% 912M/7.16G [00:10<02:05, 49.6MB/s]\u001b[A\n",
            "finetuned.gguf:  13% 933M/7.16G [00:11<01:40, 62.1MB/s]\u001b[A\n",
            "finetuned.gguf:  13% 954M/7.16G [00:11<01:24, 73.4MB/s]\u001b[A\n",
            "finetuned.gguf:  14% 975M/7.16G [00:11<01:18, 79.0MB/s]\u001b[A\n",
            "finetuned.gguf:  14% 996M/7.16G [00:11<01:07, 91.3MB/s]\u001b[A\n",
            "finetuned.gguf:  14% 1.02G/7.16G [00:11<01:03, 96.5MB/s]\u001b[A\n",
            "finetuned.gguf:  14% 1.04G/7.16G [00:11<00:55, 110MB/s] \u001b[A\n",
            "finetuned.gguf:  15% 1.06G/7.16G [00:12<01:08, 89.3MB/s]\u001b[A\n",
            "finetuned.gguf:  15% 1.08G/7.16G [00:12<01:01, 98.8MB/s]\u001b[A\n",
            "finetuned.gguf:  15% 1.10G/7.16G [00:12<00:55, 110MB/s] \u001b[A\n",
            "finetuned.gguf:  16% 1.12G/7.16G [00:12<00:51, 117MB/s]\u001b[A\n",
            "finetuned.gguf:  16% 1.14G/7.16G [00:12<00:46, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  16% 1.16G/7.16G [00:13<00:44, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  17% 1.18G/7.16G [00:13<00:43, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  17% 1.21G/7.16G [00:13<00:43, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  17% 1.23G/7.16G [00:13<00:45, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  17% 1.25G/7.16G [00:13<00:44, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  18% 1.27G/7.16G [00:13<00:43, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  18% 1.29G/7.16G [00:13<00:43, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  18% 1.31G/7.16G [00:14<00:43, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  19% 1.33G/7.16G [00:14<00:42, 138MB/s]\u001b[A\n",
            "finetuned.gguf:  19% 1.35G/7.16G [00:14<00:41, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  19% 1.37G/7.16G [00:14<00:41, 138MB/s]\u001b[A\n",
            "finetuned.gguf:  19% 1.39G/7.16G [00:14<00:41, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  20% 1.42G/7.16G [00:14<00:44, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  20% 1.44G/7.16G [00:15<00:55, 104MB/s]\u001b[A\n",
            "finetuned.gguf:  20% 1.46G/7.16G [00:15<00:50, 112MB/s]\u001b[A\n",
            "finetuned.gguf:  21% 1.48G/7.16G [00:15<00:48, 117MB/s]\u001b[A\n",
            "finetuned.gguf:  21% 1.50G/7.16G [00:15<00:45, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  21% 1.52G/7.16G [00:15<00:44, 127MB/s]\u001b[A\n",
            "finetuned.gguf:  22% 1.54G/7.16G [00:15<00:40, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  22% 1.56G/7.16G [00:16<00:40, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  22% 1.58G/7.16G [00:16<00:40, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  22% 1.60G/7.16G [00:17<02:43, 34.1MB/s]\u001b[A\n",
            "finetuned.gguf:  23% 1.63G/7.16G [00:18<02:04, 44.5MB/s]\u001b[A\n",
            "finetuned.gguf:  23% 1.65G/7.16G [00:18<01:38, 55.9MB/s]\u001b[A\n",
            "finetuned.gguf:  23% 1.67G/7.16G [00:18<01:21, 67.4MB/s]\u001b[A\n",
            "finetuned.gguf:  24% 1.69G/7.16G [00:18<01:10, 77.9MB/s]\u001b[A\n",
            "finetuned.gguf:  24% 1.71G/7.16G [00:18<01:12, 75.6MB/s]\u001b[A\n",
            "finetuned.gguf:  24% 1.73G/7.16G [00:19<01:05, 82.9MB/s]\u001b[A\n",
            "finetuned.gguf:  24% 1.75G/7.16G [00:19<00:56, 95.0MB/s]\u001b[A\n",
            "finetuned.gguf:  25% 1.77G/7.16G [00:19<00:49, 109MB/s] \u001b[A\n",
            "finetuned.gguf:  25% 1.79G/7.16G [00:19<00:44, 121MB/s]\u001b[A\n",
            "finetuned.gguf:  25% 1.81G/7.16G [00:19<00:40, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  26% 1.84G/7.16G [00:19<00:39, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  26% 1.86G/7.16G [00:19<00:39, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  26% 1.88G/7.16G [00:20<00:38, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  27% 1.90G/7.16G [00:20<00:39, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  27% 1.92G/7.16G [00:20<00:39, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  27% 1.94G/7.16G [00:20<00:38, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  27% 1.96G/7.16G [00:20<00:39, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  28% 1.98G/7.16G [00:20<00:38, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  28% 2.00G/7.16G [00:20<00:38, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  28% 2.02G/7.16G [00:21<00:38, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  29% 2.04G/7.16G [00:22<02:30, 34.0MB/s]\u001b[A\n",
            "finetuned.gguf:  29% 2.07G/7.16G [00:22<01:52, 45.2MB/s]\u001b[A\n",
            "finetuned.gguf:  29% 2.09G/7.16G [00:23<01:27, 57.8MB/s]\u001b[A\n",
            "finetuned.gguf:  29% 2.11G/7.16G [00:23<01:12, 69.5MB/s]\u001b[A\n",
            "finetuned.gguf:  30% 2.13G/7.16G [00:23<01:02, 80.5MB/s]\u001b[A\n",
            "finetuned.gguf:  30% 2.15G/7.16G [00:23<00:54, 92.6MB/s]\u001b[A\n",
            "finetuned.gguf:  30% 2.17G/7.16G [00:23<00:48, 102MB/s] \u001b[A\n",
            "finetuned.gguf:  31% 2.19G/7.16G [00:23<00:44, 111MB/s]\u001b[A\n",
            "finetuned.gguf:  31% 2.21G/7.16G [00:24<00:43, 115MB/s]\u001b[A\n",
            "finetuned.gguf:  31% 2.23G/7.16G [00:24<00:41, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  31% 2.25G/7.16G [00:24<00:39, 125MB/s]\u001b[A\n",
            "finetuned.gguf:  32% 2.28G/7.16G [00:24<00:38, 127MB/s]\u001b[A\n",
            "finetuned.gguf:  32% 2.30G/7.16G [00:24<00:46, 106MB/s]\u001b[A\n",
            "finetuned.gguf:  32% 2.32G/7.16G [00:24<00:43, 113MB/s]\u001b[A\n",
            "finetuned.gguf:  33% 2.34G/7.16G [00:25<00:40, 118MB/s]\u001b[A\n",
            "finetuned.gguf:  33% 2.36G/7.16G [00:25<00:38, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  33% 2.38G/7.16G [00:25<00:39, 121MB/s]\u001b[A\n",
            "finetuned.gguf:  34% 2.40G/7.16G [00:25<00:42, 113MB/s]\u001b[A\n",
            "finetuned.gguf:  34% 2.42G/7.16G [00:25<00:39, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  34% 2.44G/7.16G [00:25<00:35, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  34% 2.46G/7.16G [00:26<00:33, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  35% 2.49G/7.16G [00:26<00:34, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  35% 2.51G/7.16G [00:27<02:18, 33.7MB/s]\u001b[A\n",
            "finetuned.gguf:  35% 2.53G/7.16G [00:28<01:43, 44.6MB/s]\u001b[A\n",
            "finetuned.gguf:  36% 2.55G/7.16G [00:28<01:22, 55.9MB/s]\u001b[A\n",
            "finetuned.gguf:  36% 2.57G/7.16G [00:28<01:09, 66.3MB/s]\u001b[A\n",
            "finetuned.gguf:  36% 2.59G/7.16G [00:28<00:57, 79.5MB/s]\u001b[A\n",
            "finetuned.gguf:  36% 2.61G/7.16G [00:28<00:50, 90.7MB/s]\u001b[A\n",
            "finetuned.gguf:  37% 2.63G/7.16G [00:28<00:44, 102MB/s] \u001b[A\n",
            "finetuned.gguf:  37% 2.65G/7.16G [00:28<00:41, 108MB/s]\u001b[A\n",
            "finetuned.gguf:  37% 2.67G/7.16G [00:29<00:38, 116MB/s]\u001b[A\n",
            "finetuned.gguf:  38% 2.69G/7.16G [00:29<00:38, 117MB/s]\u001b[A\n",
            "finetuned.gguf:  38% 2.72G/7.16G [00:29<00:35, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  38% 2.74G/7.16G [00:29<00:34, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  39% 2.76G/7.16G [00:29<00:33, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  39% 2.78G/7.16G [00:29<00:32, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  39% 2.80G/7.16G [00:30<00:32, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  39% 2.82G/7.16G [00:30<00:32, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  40% 2.84G/7.16G [00:30<00:45, 94.5MB/s]\u001b[A\n",
            "finetuned.gguf:  40% 2.86G/7.16G [00:30<00:42, 101MB/s] \u001b[A\n",
            "finetuned.gguf:  40% 2.88G/7.16G [00:30<00:38, 110MB/s]\u001b[A\n",
            "finetuned.gguf:  41% 2.90G/7.16G [00:31<00:36, 116MB/s]\u001b[A\n",
            "finetuned.gguf:  41% 2.93G/7.16G [00:31<00:34, 123MB/s]\u001b[A\n",
            "finetuned.gguf:  41% 2.95G/7.16G [00:32<02:02, 34.3MB/s]\u001b[A\n",
            "finetuned.gguf:  42% 2.98G/7.16G [00:33<01:23, 50.1MB/s]\u001b[A\n",
            "finetuned.gguf:  42% 3.00G/7.16G [00:33<01:09, 60.2MB/s]\u001b[A\n",
            "finetuned.gguf:  42% 3.02G/7.16G [00:33<00:59, 70.0MB/s]\u001b[A\n",
            "finetuned.gguf:  42% 3.04G/7.16G [00:33<00:50, 81.1MB/s]\u001b[A\n",
            "finetuned.gguf:  43% 3.06G/7.16G [00:33<00:45, 89.8MB/s]\u001b[A\n",
            "finetuned.gguf:  43% 3.08G/7.16G [00:33<00:40, 100MB/s] \u001b[A\n",
            "finetuned.gguf:  43% 3.10G/7.16G [00:34<00:38, 105MB/s]\u001b[A\n",
            "finetuned.gguf:  44% 3.12G/7.16G [00:34<00:37, 107MB/s]\u001b[A\n",
            "finetuned.gguf:  44% 3.15G/7.16G [00:34<00:36, 109MB/s]\u001b[A\n",
            "finetuned.gguf:  44% 3.17G/7.16G [00:34<00:34, 117MB/s]\u001b[A\n",
            "finetuned.gguf:  45% 3.19G/7.16G [00:34<00:32, 122MB/s]\u001b[A\n",
            "finetuned.gguf:  45% 3.21G/7.16G [00:34<00:34, 116MB/s]\u001b[A\n",
            "finetuned.gguf:  45% 3.23G/7.16G [00:35<00:33, 116MB/s]\u001b[A\n",
            "finetuned.gguf:  45% 3.25G/7.16G [00:35<00:33, 118MB/s]\u001b[A\n",
            "finetuned.gguf:  46% 3.27G/7.16G [00:35<00:31, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  46% 3.29G/7.16G [00:35<00:30, 127MB/s]\u001b[A\n",
            "finetuned.gguf:  46% 3.31G/7.16G [00:35<00:30, 125MB/s]\u001b[A\n",
            "finetuned.gguf:  47% 3.33G/7.16G [00:35<00:29, 130MB/s]\u001b[A\n",
            "finetuned.gguf:  47% 3.36G/7.16G [00:36<00:29, 130MB/s]\u001b[A\n",
            "finetuned.gguf:  47% 3.38G/7.16G [00:36<00:29, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  47% 3.40G/7.16G [00:36<00:28, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  48% 3.42G/7.16G [00:36<00:28, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  48% 3.44G/7.16G [00:36<00:27, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  48% 3.46G/7.16G [00:36<00:27, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  49% 3.48G/7.16G [00:36<00:26, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  49% 3.50G/7.16G [00:37<00:27, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  49% 3.52G/7.16G [00:37<00:26, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  49% 3.54G/7.16G [00:37<00:26, 138MB/s]\u001b[A\n",
            "finetuned.gguf:  50% 3.57G/7.16G [00:37<00:27, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  50% 3.59G/7.16G [00:37<00:27, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  50% 3.61G/7.16G [00:37<00:27, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  51% 3.63G/7.16G [00:38<00:26, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  51% 3.65G/7.16G [00:38<00:26, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  51% 3.67G/7.16G [00:38<00:25, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  52% 3.69G/7.16G [00:38<00:25, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  52% 3.71G/7.16G [00:38<00:25, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  52% 3.73G/7.16G [00:38<00:25, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  52% 3.75G/7.16G [00:38<00:25, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  53% 3.77G/7.16G [00:39<00:25, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  53% 3.80G/7.16G [00:39<00:24, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  53% 3.82G/7.16G [00:39<00:25, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  54% 3.84G/7.16G [00:39<00:25, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  54% 3.86G/7.16G [00:39<00:25, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  54% 3.88G/7.16G [00:39<00:24, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  54% 3.90G/7.16G [00:40<00:24, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  55% 3.92G/7.16G [00:40<00:24, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  55% 3.94G/7.16G [00:40<00:23, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  55% 3.96G/7.16G [00:40<00:23, 138MB/s]\u001b[A\n",
            "finetuned.gguf:  56% 3.98G/7.16G [00:40<00:22, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  56% 4.01G/7.16G [00:40<00:23, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  56% 4.03G/7.16G [00:41<00:23, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  57% 4.05G/7.16G [00:41<00:23, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  57% 4.07G/7.16G [00:41<00:23, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  57% 4.09G/7.16G [00:41<00:23, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  57% 4.11G/7.16G [00:41<00:23, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  58% 4.13G/7.16G [00:41<00:22, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  58% 4.15G/7.16G [00:41<00:21, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  58% 4.17G/7.16G [00:42<00:50, 58.9MB/s]\u001b[A\n",
            "finetuned.gguf:  59% 4.20G/7.16G [00:42<00:36, 80.7MB/s]\u001b[A\n",
            "finetuned.gguf:  59% 4.23G/7.16G [00:43<00:32, 89.3MB/s]\u001b[A\n",
            "finetuned.gguf:  59% 4.25G/7.16G [00:43<00:30, 96.1MB/s]\u001b[A\n",
            "finetuned.gguf:  60% 4.27G/7.16G [00:43<00:27, 105MB/s] \u001b[A\n",
            "finetuned.gguf:  60% 4.29G/7.16G [00:43<00:25, 113MB/s]\u001b[A\n",
            "finetuned.gguf:  60% 4.31G/7.16G [00:43<00:23, 119MB/s]\u001b[A\n",
            "finetuned.gguf:  60% 4.33G/7.16G [00:43<00:22, 125MB/s]\u001b[A\n",
            "finetuned.gguf:  61% 4.35G/7.16G [00:44<00:21, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  61% 4.37G/7.16G [00:44<00:21, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  61% 4.39G/7.16G [00:44<00:23, 119MB/s]\u001b[A\n",
            "finetuned.gguf:  62% 4.41G/7.16G [00:44<00:23, 118MB/s]\u001b[A\n",
            "finetuned.gguf:  62% 4.44G/7.16G [00:44<00:22, 122MB/s]\u001b[A\n",
            "finetuned.gguf:  62% 4.46G/7.16G [00:44<00:20, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  63% 4.48G/7.16G [00:45<00:21, 127MB/s]\u001b[A\n",
            "finetuned.gguf:  63% 4.50G/7.16G [00:45<00:20, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  63% 4.52G/7.16G [00:45<00:20, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  63% 4.54G/7.16G [00:45<00:19, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  64% 4.56G/7.16G [00:45<00:20, 127MB/s]\u001b[A\n",
            "finetuned.gguf:  64% 4.58G/7.16G [00:45<00:20, 125MB/s]\u001b[A\n",
            "finetuned.gguf:  64% 4.60G/7.16G [00:45<00:18, 140MB/s]\u001b[A\n",
            "finetuned.gguf:  65% 4.62G/7.16G [00:47<01:19, 32.1MB/s]\u001b[A\n",
            "finetuned.gguf:  65% 4.65G/7.16G [00:47<00:59, 42.3MB/s]\u001b[A\n",
            "finetuned.gguf:  65% 4.67G/7.16G [00:48<00:54, 45.9MB/s]\u001b[A\n",
            "finetuned.gguf:  65% 4.69G/7.16G [00:48<00:42, 58.4MB/s]\u001b[A\n",
            "finetuned.gguf:  66% 4.71G/7.16G [00:48<00:34, 71.9MB/s]\u001b[A\n",
            "finetuned.gguf:  66% 4.73G/7.16G [00:48<00:28, 84.2MB/s]\u001b[A\n",
            "finetuned.gguf:  66% 4.75G/7.16G [00:48<00:25, 95.2MB/s]\u001b[A\n",
            "finetuned.gguf:  67% 4.77G/7.16G [00:49<00:24, 98.9MB/s]\u001b[A\n",
            "finetuned.gguf:  67% 4.79G/7.16G [00:49<00:23, 100MB/s] \u001b[A\n",
            "finetuned.gguf:  67% 4.81G/7.16G [00:49<00:20, 113MB/s]\u001b[A\n",
            "finetuned.gguf:  68% 4.83G/7.16G [00:49<00:19, 119MB/s]\u001b[A\n",
            "finetuned.gguf:  68% 4.85G/7.16G [00:49<00:20, 115MB/s]\u001b[A\n",
            "finetuned.gguf:  68% 4.88G/7.16G [00:49<00:17, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  68% 4.90G/7.16G [00:50<00:17, 130MB/s]\u001b[A\n",
            "finetuned.gguf:  69% 4.92G/7.16G [00:50<00:16, 139MB/s]\u001b[A\n",
            "finetuned.gguf:  69% 4.94G/7.16G [00:50<00:18, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  69% 4.96G/7.16G [00:50<00:16, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  70% 4.98G/7.16G [00:50<00:15, 140MB/s]\u001b[A\n",
            "finetuned.gguf:  70% 5.00G/7.16G [00:50<00:16, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  70% 5.02G/7.16G [00:50<00:15, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  70% 5.04G/7.16G [00:53<01:13, 28.9MB/s]\u001b[A\n",
            "finetuned.gguf:  71% 5.06G/7.16G [00:53<00:56, 37.2MB/s]\u001b[A\n",
            "finetuned.gguf:  71% 5.09G/7.16G [00:53<00:43, 47.6MB/s]\u001b[A\n",
            "finetuned.gguf:  71% 5.11G/7.16G [00:53<00:34, 59.9MB/s]\u001b[A\n",
            "finetuned.gguf:  72% 5.13G/7.16G [00:53<00:29, 67.8MB/s]\u001b[A\n",
            "finetuned.gguf:  72% 5.15G/7.16G [00:53<00:26, 77.3MB/s]\u001b[A\n",
            "finetuned.gguf:  72% 5.17G/7.16G [00:54<00:24, 81.4MB/s]\u001b[A\n",
            "finetuned.gguf:  72% 5.19G/7.16G [00:54<00:22, 89.1MB/s]\u001b[A\n",
            "finetuned.gguf:  73% 5.21G/7.16G [00:54<00:22, 87.9MB/s]\u001b[A\n",
            "finetuned.gguf:  73% 5.23G/7.16G [00:54<00:21, 91.6MB/s]\u001b[A\n",
            "finetuned.gguf:  73% 5.25G/7.16G [00:54<00:18, 103MB/s] \u001b[A\n",
            "finetuned.gguf:  74% 5.27G/7.16G [00:55<00:16, 118MB/s]\u001b[A\n",
            "finetuned.gguf:  74% 5.30G/7.16G [00:55<00:15, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  74% 5.32G/7.16G [00:55<00:13, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  75% 5.34G/7.16G [00:55<00:13, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  75% 5.36G/7.16G [00:56<00:46, 38.7MB/s]\u001b[A\n",
            "finetuned.gguf:  75% 5.38G/7.16G [00:57<00:35, 50.2MB/s]\u001b[A\n",
            "finetuned.gguf:  75% 5.40G/7.16G [00:57<00:27, 63.7MB/s]\u001b[A\n",
            "finetuned.gguf:  76% 5.42G/7.16G [00:57<00:22, 77.6MB/s]\u001b[A\n",
            "finetuned.gguf:  76% 5.44G/7.16G [00:57<00:19, 88.5MB/s]\u001b[A\n",
            "finetuned.gguf:  76% 5.46G/7.16G [00:57<00:17, 98.0MB/s]\u001b[A\n",
            "finetuned.gguf:  77% 5.48G/7.16G [00:57<00:16, 98.9MB/s]\u001b[A\n",
            "finetuned.gguf:  77% 5.51G/7.16G [00:57<00:15, 104MB/s] \u001b[A\n",
            "finetuned.gguf:  77% 5.53G/7.16G [00:58<00:14, 112MB/s]\u001b[A\n",
            "finetuned.gguf:  77% 5.55G/7.16G [00:58<00:14, 115MB/s]\u001b[A\n",
            "finetuned.gguf:  78% 5.57G/7.16G [00:58<00:12, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  78% 5.59G/7.16G [00:58<00:11, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  78% 5.61G/7.16G [00:58<00:11, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  79% 5.63G/7.16G [00:58<00:11, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  79% 5.65G/7.16G [00:59<00:11, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  79% 5.67G/7.16G [00:59<00:10, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  80% 5.69G/7.16G [00:59<00:11, 123MB/s]\u001b[A\n",
            "finetuned.gguf:  80% 5.71G/7.16G [00:59<00:11, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  80% 5.74G/7.16G [00:59<00:10, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  80% 5.76G/7.16G [00:59<00:09, 142MB/s]\u001b[A\n",
            "finetuned.gguf:  81% 5.78G/7.16G [00:59<00:09, 140MB/s]\u001b[A\n",
            "finetuned.gguf:  81% 5.80G/7.16G [01:01<00:45, 29.8MB/s]\u001b[A\n",
            "finetuned.gguf:  81% 5.82G/7.16G [01:02<00:33, 39.5MB/s]\u001b[A\n",
            "finetuned.gguf:  82% 5.84G/7.16G [01:02<00:25, 50.9MB/s]\u001b[A\n",
            "finetuned.gguf:  82% 5.86G/7.16G [01:02<00:21, 59.4MB/s]\u001b[A\n",
            "finetuned.gguf:  82% 5.88G/7.16G [01:02<00:19, 66.8MB/s]\u001b[A\n",
            "finetuned.gguf:  82% 5.90G/7.16G [01:02<00:17, 72.4MB/s]\u001b[A\n",
            "finetuned.gguf:  83% 5.92G/7.16G [01:03<00:15, 77.5MB/s]\u001b[A\n",
            "finetuned.gguf:  83% 5.95G/7.16G [01:03<00:14, 85.5MB/s]\u001b[A\n",
            "finetuned.gguf:  83% 5.97G/7.16G [01:03<00:11, 101MB/s] \u001b[A\n",
            "finetuned.gguf:  84% 5.99G/7.16G [01:03<00:10, 113MB/s]\u001b[A\n",
            "finetuned.gguf:  84% 6.01G/7.16G [01:03<00:09, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  84% 6.03G/7.16G [01:03<00:09, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  84% 6.05G/7.16G [01:04<00:08, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  85% 6.07G/7.16G [01:04<00:08, 131MB/s]\u001b[A\n",
            "finetuned.gguf:  85% 6.09G/7.16G [01:04<00:08, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  85% 6.11G/7.16G [01:04<00:07, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  86% 6.13G/7.16G [01:04<00:07, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  86% 6.16G/7.16G [01:04<00:07, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  86% 6.18G/7.16G [01:04<00:07, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  87% 6.20G/7.16G [01:05<00:07, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  87% 6.22G/7.16G [01:05<00:07, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  87% 6.24G/7.16G [01:06<00:16, 57.4MB/s]\u001b[A\n",
            "finetuned.gguf:  88% 6.27G/7.16G [01:06<00:11, 78.5MB/s]\u001b[A\n",
            "finetuned.gguf:  88% 6.29G/7.16G [01:06<00:09, 87.8MB/s]\u001b[A\n",
            "finetuned.gguf:  88% 6.31G/7.16G [01:06<00:08, 99.4MB/s]\u001b[A\n",
            "finetuned.gguf:  88% 6.33G/7.16G [01:06<00:07, 107MB/s] \u001b[A\n",
            "finetuned.gguf:  89% 6.35G/7.16G [01:06<00:07, 112MB/s]\u001b[A\n",
            "finetuned.gguf:  89% 6.38G/7.16G [01:07<00:06, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  89% 6.40G/7.16G [01:07<00:06, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  90% 6.42G/7.16G [01:07<00:05, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  90% 6.44G/7.16G [01:07<00:05, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  90% 6.46G/7.16G [01:07<00:05, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  90% 6.48G/7.16G [01:07<00:05, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  91% 6.50G/7.16G [01:07<00:04, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  91% 6.52G/7.16G [01:08<00:04, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  91% 6.54G/7.16G [01:08<00:04, 138MB/s]\u001b[A\n",
            "finetuned.gguf:  92% 6.56G/7.16G [01:08<00:04, 137MB/s]\u001b[A\n",
            "finetuned.gguf:  92% 6.59G/7.16G [01:08<00:04, 138MB/s]\u001b[A\n",
            "finetuned.gguf:  92% 6.61G/7.16G [01:08<00:04, 136MB/s]\u001b[A\n",
            "finetuned.gguf:  93% 6.63G/7.16G [01:08<00:03, 135MB/s]\u001b[A\n",
            "finetuned.gguf:  93% 6.65G/7.16G [01:09<00:04, 128MB/s]\u001b[A\n",
            "finetuned.gguf:  93% 6.67G/7.16G [01:09<00:03, 126MB/s]\u001b[A\n",
            "finetuned.gguf:  93% 6.69G/7.16G [01:09<00:03, 126MB/s]\u001b[A\n",
            "finetuned.gguf:  94% 6.71G/7.16G [01:09<00:03, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  94% 6.73G/7.16G [01:09<00:03, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  94% 6.75G/7.16G [01:09<00:03, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  95% 6.77G/7.16G [01:12<00:13, 28.0MB/s]\u001b[A\n",
            "finetuned.gguf:  95% 6.79G/7.16G [01:12<00:09, 37.5MB/s]\u001b[A\n",
            "finetuned.gguf:  95% 6.82G/7.16G [01:12<00:07, 47.3MB/s]\u001b[A\n",
            "finetuned.gguf:  95% 6.84G/7.16G [01:12<00:05, 57.0MB/s]\u001b[A\n",
            "finetuned.gguf:  96% 6.86G/7.16G [01:12<00:04, 70.0MB/s]\u001b[A\n",
            "finetuned.gguf:  96% 6.88G/7.16G [01:12<00:03, 82.4MB/s]\u001b[A\n",
            "finetuned.gguf:  96% 6.90G/7.16G [01:12<00:02, 94.3MB/s]\u001b[A\n",
            "finetuned.gguf:  97% 6.92G/7.16G [01:13<00:02, 104MB/s] \u001b[A\n",
            "finetuned.gguf:  97% 6.94G/7.16G [01:13<00:01, 112MB/s]\u001b[A\n",
            "finetuned.gguf:  97% 6.96G/7.16G [01:13<00:01, 120MB/s]\u001b[A\n",
            "finetuned.gguf:  98% 6.98G/7.16G [01:13<00:01, 124MB/s]\u001b[A\n",
            "finetuned.gguf:  98% 7.00G/7.16G [01:13<00:01, 129MB/s]\u001b[A\n",
            "finetuned.gguf:  98% 7.03G/7.16G [01:13<00:01, 132MB/s]\u001b[A\n",
            "finetuned.gguf:  98% 7.05G/7.16G [01:13<00:00, 133MB/s]\u001b[A\n",
            "finetuned.gguf:  99% 7.07G/7.16G [01:14<00:00, 130MB/s]\u001b[A\n",
            "finetuned.gguf:  99% 7.09G/7.16G [01:14<00:00, 134MB/s]\u001b[A\n",
            "finetuned.gguf:  99% 7.11G/7.16G [01:14<00:00, 130MB/s]\u001b[A\n",
            "finetuned.gguf: 100% 7.13G/7.16G [01:14<00:00, 133MB/s]\u001b[A\n",
            "finetuned.gguf: 100% 7.16G/7.16G [01:14<00:00, 95.6MB/s]\n",
            "Fetching 1 files: 100% 1/1 [01:15<00:00, 75.41s/it]\n",
            "2024-02-29 07:07:32 - Created default chainlit markdown file at /content/chainlit.md\n",
            "2024-02-29 07:07:32 - Your app is available at http://localhost:8000\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `acall` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use ainvoke instead.\n",
            "  warn_deprecated(\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "[INST]\n",
            "<<SYS>>\n",
            "As a chatbot for the Company \"Maitri Lab Grown Diamonds,\" your primary focus is to provide concise, accurate, and helpful information about the company and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise, or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
            "Be humble.\n",
            "Respond to the following question as a chatbot for the company named \"MAITRI LAB GROWN DIAMONDS.\"\n",
            "<</SYS>>\n",
            "\n",
            "Hey there![/INST]\u001b[0m\n",
            "  Hello! *adjusts glasses* It's a pleasure to assist you with any inquiries you may have about Maitri Lab Grown Diamonds. How can I help you today? Are you interested in learning more about our company, the process of growing diamonds, or perhaps you have questions about the different types of lab-grown diamonds we offer? Please feel free to ask, and I'll do my best to provide you with accurate and helpful information.\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "[INST]\n",
            "<<SYS>>\n",
            "As a chatbot for the Company \"Maitri Lab Grown Diamonds,\" your primary focus is to provide concise, accurate, and helpful information about the company and the diamond industry. Engage in discussions related to diamonds, including sourcing, types, and care, as well as information specific to Maitri Diamonds. When asked about unrelated topics, politely redirect the conversation to the company, your areas of expertise, or inform the user that the topic falls outside your scope. Maintain a professional and neutral tone, respecting all viewpoints and ensuring accuracy in your responses.\n",
            "Be humble.\n",
            "Respond to the following question as a chatbot for the company named \"MAITRI LAB GROWN DIAMONDS.\"\n",
            "<</SYS>>\n",
            "\n",
            "Tell me about your company.[/INST]\u001b[0m\n",
            "  Hello! As a chatbot for Maitri Lab Grown Diamonds, I'd be delighted to share information about our company. Maitri Lab Grown Diamonds is a leading diamond manufacturer and supplier based in India, known for producing high-quality, ethically sourced diamonds.\n",
            "\n",
            "At Maitri Lab Grown Diamonds, we are committed to sustainability and ethical practices throughout our supply chain. We source our diamonds from reputable suppliers who adhere to strict ethical standards, ensuring that every diamond is conflict-free and environmentally responsible. Our team of experts carefully selects each diamond based on its quality, cut, color, and clarity to offer a wide range of options for our customers.\n",
            "\n",
            "We take pride in offering exceptional customer service and providing our clients with personalized support throughout the purchasing process. Whether you're looking for a unique engagement ring or a special occasion piece, our team is here to help you find the perfect diamond that meets your needs and budget.\n",
            "\n",
            "In addition to our high-quality diamonds, we also offer a range of services to help you care for and maintain your investment. From diamond cleaning and polishing to jewelry repair and restoration, we have everything you need to keep your diamond looking its best.\n",
            "\n",
            "At Maitri Lab Grown Diamonds, we are passionate about providing our customers with the best possible experience. We believe that buying a diamond should be an exciting and stress-free process, which is why we offer a 30-day money-back guarantee and free shipping on all orders.\n",
            "\n",
            "I hope this information has been helpful! Is there anything else you would like to know about Maitri Lab Grown Diamonds or the diamond industry in general?\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "kUbafI-JassW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In this notebook, we explored how can you finetune the Llama 2 chat model as per your needs. It included everything from the very basic creation of the dataset to the frontend. You can easily now deploy it using Hugging face spaces, or any platform that you like.\n",
        "\n",
        "If you've understood the code in this notebook, you will be able to finetune any other model."
      ],
      "metadata": {
        "id": "I4OkMWzlaT5M"
      }
    }
  ]
}